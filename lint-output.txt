[{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\ACC-01-phone-normalization.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\ACC-01-phone-normalization.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { PhoneNormalizeStep } from '../../transforms/transform-chain.js';\n\ndescribe('ACC-01: E.164 phone normalization', () => {\n  const step = new PhoneNormalizeStep('MY');\n\n  it('should normalize Malaysian local number to E.164', () => {\n    expect(step.transform('012-3456789', {} as never)).toBe('+60123456789');\n  });\n\n  it('should be idempotent for already-E.164 numbers', () => {\n    expect(step.transform('+60123456789', {} as never)).toBe('+60123456789');\n  });\n\n  it('should normalize Malaysian number with spaces', () => {\n    expect(step.transform('012 345 6789', {} as never)).toBe('+60123456789');\n  });\n\n  it('should normalize Malaysian number with parentheses', () => {\n    expect(step.transform('(012) 3456789', {} as never)).toBe('+60123456789');\n  });\n\n  it('should return null for invalid phone numbers', () => {\n    expect(step.transform('not-a-phone', {} as never)).toBeNull();\n  });\n\n  it('should return null for too-short numbers', () => {\n    expect(step.transform('123', {} as never)).toBeNull();\n  });\n\n  it('should pass through non-string values', () => {\n    expect(step.transform(42, {} as never)).toBe(42);\n    expect(step.transform(null, {} as never)).toBeNull();\n    expect(step.transform(undefined, {} as never)).toBeUndefined();\n  });\n\n  it('should pass through empty strings', () => {\n    expect(step.transform('', {} as never)).toBe('');\n    expect(step.transform('   ', {} as never)).toBe('   ');\n  });\n\n  it('should respect custom default region', () => {\n    const usStep = new PhoneNormalizeStep('US');\n    expect(usStep.transform('(213) 373-4253', {} as never)).toBe('+12133734253');\n  });\n\n  it('should only handle phone data type', () => {\n    expect(step.canHandle('phone', 'phone')).toBe(true);\n    expect(step.canHandle('phone', 'short_text')).toBe(false);\n    expect(step.canHandle('phone', 'email')).toBe(false);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\ACC-02-email-normalization.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\ACC-02-email-normalization.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { EmailNormalizeStep } from '../../transforms/transform-chain.js';\n\ndescribe('ACC-02: Smart email normalization', () => {\n  const step = new EmailNormalizeStep();\n\n  // Gmail dot stripping\n  it('should strip dots from Gmail local part', () => {\n    expect(step.transform('john.doe@gmail.com', {} as never)).toBe('johndoe@gmail.com');\n  });\n\n  it('should strip dots from googlemail.com local part', () => {\n    expect(step.transform('john.doe@googlemail.com', {} as never)).toBe('johndoe@googlemail.com');\n  });\n\n  // Gmail plus alias stripping\n  it('should strip + alias from Gmail', () => {\n    expect(step.transform('john+tag@gmail.com', {} as never)).toBe('john@gmail.com');\n  });\n\n  it('should strip dots AND + alias from Gmail', () => {\n    expect(step.transform('j.o.h.n+newsletter@gmail.com', {} as never)).toBe('john@gmail.com');\n  });\n\n  // Non-Gmail: unchanged (except lowercase)\n  it('should NOT strip dots from non-Gmail domains', () => {\n    expect(step.transform('john.doe@company.com', {} as never)).toBe('john.doe@company.com');\n  });\n\n  it('should NOT strip + alias from non-Gmail domains', () => {\n    expect(step.transform('john+tag@company.com', {} as never)).toBe('john+tag@company.com');\n  });\n\n  // Case normalization\n  it('should always lowercase', () => {\n    expect(step.transform('John.Doe@Gmail.COM', {} as never)).toBe('johndoe@gmail.com');\n  });\n\n  it('should lowercase non-Gmail emails', () => {\n    expect(step.transform('ADMIN@Company.COM', {} as never)).toBe('admin@company.com');\n  });\n\n  // Trim\n  it('should trim whitespace', () => {\n    expect(step.transform('  john@gmail.com  ', {} as never)).toBe('john@gmail.com');\n  });\n\n  // Edge cases\n  it('should handle email without @ sign', () => {\n    expect(step.transform('noemail', {} as never)).toBe('noemail');\n  });\n\n  it('should pass through non-string values', () => {\n    expect(step.transform(42, {} as never)).toBe(42);\n    expect(step.transform(null, {} as never)).toBeNull();\n  });\n\n  it('should only handle email data type', () => {\n    expect(step.canHandle('email', 'email')).toBe(true);\n    expect(step.canHandle('email', 'phone')).toBe(false);\n    expect(step.canHandle('email', 'short_text')).toBe(false);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\ACC-03-fuzzy-name.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\ACC-03-fuzzy-name.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { fuzzyMatchName, batchFuzzyMatchNames } from '../../strategies/fuzzy-name-matcher.js';\n\ndescribe('ACC-03: Fuzzy name matching with Fuse.js', () => {\n  const candidates = [\n    { index: 0, value: 'Alice Johnson' },\n    { index: 1, value: 'Bob Smith' },\n    { index: 2, value: 'Charlie Brown' },\n    { index: 3, value: 'Alicia Johnston' },\n  ];\n\n  it('should find exact name matches with high score', () => {\n    const results = fuzzyMatchName('Alice Johnson', candidates);\n    expect(results.length).toBeGreaterThan(0);\n    expect(results[0]!.candidateValue).toBe('Alice Johnson');\n    expect(results[0]!.normalizedScore).toBeGreaterThan(15);\n    expect(results[0]!.explanation.matchType).toBe('fuzzy');\n    expect(results[0]!.explanation.field).toBe('name');\n  });\n\n  it('should find similar names (typo tolerance)', () => {\n    const results = fuzzyMatchName('Alise Johnson', candidates);\n    expect(results.length).toBeGreaterThan(0);\n    // Should match Alice Johnson or Alicia Johnston\n    const matchedValues = results.map(r => r.candidateValue);\n    expect(matchedValues).toContain('Alice Johnson');\n  });\n\n  it('should find phonetically similar names', () => {\n    const results = fuzzyMatchName('Alicia Johnston', candidates);\n    expect(results.length).toBeGreaterThan(0);\n    expect(results[0]!.candidateValue).toBe('Alicia Johnston');\n    expect(results[0]!.normalizedScore).toBe(20); // perfect match\n  });\n\n  it('should return empty for completely different names', () => {\n    const results = fuzzyMatchName('Zephyr Moonstone', candidates, { threshold: 0.3 });\n    // With a strict threshold, very different names should not match\n    expect(results.length).toBe(0);\n  });\n\n  it('should return empty for empty input', () => {\n    expect(fuzzyMatchName('', candidates)).toHaveLength(0);\n    expect(fuzzyMatchName('Alice', [])).toHaveLength(0);\n  });\n\n  it('should respect custom threshold', () => {\n    // Very strict threshold ÔÇö only near-exact matches\n    const strict = fuzzyMatchName('Alise Johnson', candidates, { threshold: 0.1 });\n    // Lenient threshold ÔÇö more matches\n    const lenient = fuzzyMatchName('Alise Johnson', candidates, { threshold: 0.6 });\n    expect(lenient.length).toBeGreaterThanOrEqual(strict.length);\n  });\n\n  it('should respect custom score weight', () => {\n    const low = fuzzyMatchName('Alice Johnson', candidates, { scoreWeight: 10 });\n    const high = fuzzyMatchName('Alice Johnson', candidates, { scoreWeight: 50 });\n    expect(low[0]!.normalizedScore).toBeLessThanOrEqual(10);\n    expect(high[0]!.normalizedScore).toBeLessThanOrEqual(50);\n    expect(high[0]!.normalizedScore).toBeGreaterThan(low[0]!.normalizedScore);\n  });\n\n  it('should produce valid MatchExplanation shape', () => {\n    const results = fuzzyMatchName('Bob Smith', candidates);\n    expect(results.length).toBeGreaterThan(0);\n    const expl = results[0]!.explanation;\n    expect(expl).toHaveProperty('field');\n    expect(expl).toHaveProperty('matchType');\n    expect(expl).toHaveProperty('scoreContribution');\n    expect(expl).toHaveProperty('legacyValue');\n    expect(expl).toHaveProperty('candidateValue');\n    expect(expl.matchType).toBe('fuzzy');\n    expect(expl.legacyValue).toBe('Bob Smith');\n    expect(expl.candidateValue).toBe('Bob Smith');\n  });\n\n  describe('batchFuzzyMatchNames', () => {\n    it('should match multiple legacy names against candidates', () => {\n      const legacyNames = [\n        { index: 0, value: 'Alice Johnson' },\n        { index: 1, value: 'Bob Smyth' },\n        { index: 2, value: 'Unknown Person' },\n      ];\n\n      const resultMap = batchFuzzyMatchNames(legacyNames, candidates);\n\n      // Alice should match\n      expect(resultMap.has(0)).toBe(true);\n      expect(resultMap.get(0)![0]!.candidateValue).toBe('Alice Johnson');\n\n      // Bob Smyth should fuzzy-match Bob Smith\n      expect(resultMap.has(1)).toBe(true);\n      const bobMatches = resultMap.get(1)!;\n      expect(bobMatches.some(m => m.candidateValue === 'Bob Smith')).toBe(true);\n    });\n\n    it('should skip empty legacy names', () => {\n      const legacyNames = [\n        { index: 0, value: '' },\n        { index: 1, value: 'Alice Johnson' },\n      ];\n\n      const resultMap = batchFuzzyMatchNames(legacyNames, candidates);\n      expect(resultMap.has(0)).toBe(false);\n      expect(resultMap.has(1)).toBe(true);\n    });\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\ACC-05-match-explanations.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\ACC-05-match-explanations.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { ContactsConflictDetector, InvoicesConflictDetector, ProductsConflictDetector } from '../../strategies/conflict-detector.js';\n\nimport type { DetectorQueryFn } from '../../strategies/conflict-detector.js';\nimport type { TransformedRecord } from '../../types/migration-job.js';\n\ndescribe('ACC-05: Match explanations end-to-end', () => {\n  const orgId = 'org-1';\n\n  describe('ContactsConflictDetector', () => {\n    const detector = new ContactsConflictDetector();\n\n    const makeQueryFn = (candidates: Array<Record<string, unknown>>): DetectorQueryFn =>\n      async () => candidates;\n\n    it('should produce email explanation with score contribution', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { email: 'alice@example.com', phone: null } },\n      ];\n      const queryFn = makeQueryFn([\n        { id: 'A1', email: 'alice@example.com', phone: null, name: 'Alice' },\n      ]);\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      expect(conflicts).toHaveLength(1);\n\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toBeDefined();\n      expect(match.explanations).toHaveLength(1);\n      expect(match.explanations![0]).toEqual({\n        field: 'email',\n        matchType: 'normalized',\n        scoreContribution: 40,\n        legacyValue: 'alice@example.com',\n        candidateValue: 'alice@example.com',\n      });\n      expect(match.score).toBe(40);\n    });\n\n    it('should produce both email + phone explanations when both match', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { email: 'bob@test.com', phone: '+60123456789' } },\n      ];\n      const queryFn = makeQueryFn([\n        { id: 'B1', email: 'bob@test.com', phone: '+60123456789', name: 'Bob' },\n      ]);\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      expect(conflicts).toHaveLength(1);\n\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toHaveLength(2);\n      expect(match.score).toBe(60);\n\n      const emailExpl = match.explanations!.find(e => e.field === 'email');\n      const phoneExpl = match.explanations!.find(e => e.field === 'phone');\n      expect(emailExpl!.scoreContribution).toBe(40);\n      expect(phoneExpl!.scoreContribution).toBe(20);\n      expect(emailExpl!.matchType).toBe('normalized');\n      expect(phoneExpl!.matchType).toBe('exact');\n    });\n\n    it('should produce phone-only explanation when only phone matches', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { email: 'different@test.com', phone: '+60111111111' } },\n      ];\n      const queryFn = makeQueryFn([\n        { id: 'C1', email: 'other@test.com', phone: '+60111111111', name: 'Charlie' },\n      ]);\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      expect(conflicts).toHaveLength(1);\n\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toHaveLength(1);\n      expect(match.explanations![0]!.field).toBe('phone');\n      expect(match.score).toBe(20);\n    });\n  });\n\n  describe('InvoicesConflictDetector', () => {\n    const detector = new InvoicesConflictDetector();\n\n    it('should produce invoice number + vendor explanations', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { invoiceNumber: 'INV-001', vendorId: 'V1' } },\n      ];\n      const queryFn: DetectorQueryFn = async () => [\n        { id: 'I1', invoiceNumber: 'INV-001', vendorId: 'V1' },\n      ];\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      expect(conflicts).toHaveLength(1);\n\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toHaveLength(2);\n      expect(match.score).toBe(90);\n\n      const invExpl = match.explanations!.find(e => e.field === 'invoiceNumber');\n      const vendorExpl = match.explanations!.find(e => e.field === 'vendorId');\n      expect(invExpl!.scoreContribution).toBe(50);\n      expect(vendorExpl!.scoreContribution).toBe(40);\n    });\n\n    it('should produce invoice-only explanation when vendor differs', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { invoiceNumber: 'INV-002', vendorId: 'V1' } },\n      ];\n      const queryFn: DetectorQueryFn = async () => [\n        { id: 'I2', invoiceNumber: 'INV-002', vendorId: 'V2' },\n      ];\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toHaveLength(1);\n      expect(match.explanations![0]!.field).toBe('invoiceNumber');\n      expect(match.score).toBe(50);\n    });\n  });\n\n  describe('ProductsConflictDetector', () => {\n    const detector = new ProductsConflictDetector();\n\n    it('should produce SKU explanation with 100 score', async () => {\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { sku: 'SKU-001', name: 'Widget' } },\n      ];\n      const queryFn: DetectorQueryFn = async () => [\n        { id: 'P1', sku: 'SKU-001', name: 'Widget A' },\n      ];\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      expect(conflicts).toHaveLength(1);\n\n      const match = conflicts[0]!.matches[0]!;\n      expect(match.explanations).toHaveLength(1);\n      expect(match.explanations![0]).toEqual({\n        field: 'sku',\n        matchType: 'exact',\n        scoreContribution: 100,\n        legacyValue: 'SKU-001',\n        candidateValue: 'SKU-001',\n      });\n      expect(match.score).toBe(100);\n    });\n  });\n\n  describe('Explanation structure invariants', () => {\n    it('should have scoreContribution sum equal to total score', async () => {\n      const detector = new ContactsConflictDetector();\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { email: 'test@test.com', phone: '+60123456789' } },\n      ];\n      const queryFn: DetectorQueryFn = async () => [\n        { id: 'X1', email: 'test@test.com', phone: '+60123456789', name: 'Test' },\n      ];\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      const match = conflicts[0]!.matches[0]!;\n\n      const explanationSum = match.explanations!.reduce((sum, e) => sum + e.scoreContribution, 0);\n      expect(explanationSum).toBe(match.score);\n    });\n\n    it('should have valid matchType values', async () => {\n      const detector = new ContactsConflictDetector();\n      const records: TransformedRecord[] = [\n        { legacyId: 'L1', data: { email: 'a@b.com', phone: '+1234' } },\n      ];\n      const queryFn: DetectorQueryFn = async () => [\n        { id: 'Y1', email: 'a@b.com', phone: '+1234', name: 'Y' },\n      ];\n\n      const conflicts = await detector.detectBulk(records, { orgId, queryFn });\n      const match = conflicts[0]!.matches[0]!;\n\n      for (const expl of match.explanations!) {\n        expect(['exact', 'normalized', 'fuzzy']).toContain(expl.matchType);\n      }\n    });\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\GOV-01-perf-budget.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\GOV-01-perf-budget.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { PerfTracker } from '../../pipeline/perf-tracker.js';\nimport { withTerminalOutcome } from '../../pipeline/with-terminal-outcome.js';\n\nimport type { RecordOutcome } from '../../types/record-outcome.js';\n\ndescribe('GOV-01: Performance budget assertions', () => {\n  it('should track p50 and p95 latencies accurately', () => {\n    const perf = new PerfTracker();\n\n    // Simulate 100 latency samples: 1ms to 100ms\n    for (let i = 1; i <= 100; i++) {\n      perf.record('mutate_create_ms', i);\n    }\n\n    const report = perf.report();\n    const createMetric = report.find(r => r.label === 'mutate_create_ms');\n    expect(createMetric).toBeDefined();\n    expect(createMetric!.count).toBe(100);\n    expect(createMetric!.p50).toBe(51);\n    expect(createMetric!.p95).toBe(96);\n  });\n\n  it('should keep withTerminalOutcome overhead under 5ms for successful operations', async () => {\n    const iterations = 50;\n    const latencies: number[] = [];\n\n    for (let i = 0; i < iterations; i++) {\n      const start = performance.now();\n      await withTerminalOutcome(\n        { entityType: 'contacts', legacyId: `L${i}` },\n        async (): Promise<RecordOutcome> => ({\n          entityType: 'contacts',\n          legacyId: `L${i}`,\n          status: 'loaded',\n          action: 'create',\n        }),\n      );\n      latencies.push(performance.now() - start);\n    }\n\n    latencies.sort((a, b) => a - b);\n    const p95 = latencies[Math.floor(latencies.length * 0.95)]!;\n\n    // withTerminalOutcome overhead should be negligible for successful ops\n    expect(p95).toBeLessThan(5);\n  });\n\n  it('should keep PerfTracker.start/end overhead under 1ms', () => {\n    const perf = new PerfTracker();\n    const iterations = 1000;\n    const overheads: number[] = [];\n\n    for (let i = 0; i < iterations; i++) {\n      const outerStart = performance.now();\n      const end = perf.start('test_label');\n      end();\n      overheads.push(performance.now() - outerStart);\n    }\n\n    overheads.sort((a, b) => a - b);\n    const p95 = overheads[Math.floor(overheads.length * 0.95)]!;\n\n    expect(p95).toBeLessThan(1);\n    expect(perf.report().find(r => r.label === 'test_label')!.count).toBe(iterations);\n  });\n\n  it('should produce correct report shape', () => {\n    const perf = new PerfTracker();\n    perf.record('extract_batch_ms', 10);\n    perf.record('extract_batch_ms', 20);\n    perf.record('detect_conflicts_ms', 5);\n\n    const report = perf.report();\n    expect(report).toHaveLength(2);\n\n    for (const metric of report) {\n      expect(metric).toHaveProperty('label');\n      expect(metric).toHaveProperty('count');\n      expect(metric).toHaveProperty('p50');\n      expect(metric).toHaveProperty('p95');\n      expect(typeof metric.label).toBe('string');\n      expect(typeof metric.count).toBe('number');\n      expect(typeof metric.p50).toBe('number');\n      expect(typeof metric.p95).toBe('number');\n      expect(metric.p50).toBeLessThanOrEqual(metric.p95);\n    }\n  });\n\n  it('should handle single-sample metrics', () => {\n    const perf = new PerfTracker();\n    perf.record('single_ms', 42);\n\n    const report = perf.report();\n    const metric = report.find(r => r.label === 'single_ms')!;\n    expect(metric.count).toBe(1);\n    expect(metric.p50).toBe(42);\n    expect(metric.p95).toBe(42);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\GOV-02-chaos.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\GOV-02-chaos.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect, vi } from 'vitest';\n\nimport { withTerminalOutcome, classifyError } from '../../pipeline/with-terminal-outcome.js';\nimport type { RecordOutcome } from '../../types/record-outcome.js';\n\ndescribe('GOV-02: Chaos / fault injection tests', () => {\n  const base = { entityType: 'contacts', legacyId: 'L1' };\n\n  it('should quarantine after mutate() timeout (transient ÔåÆ retry ÔåÆ quarantine)', async () => {\n    const fn = vi.fn().mockRejectedValue(\n      Object.assign(new Error('socket hang up'), { stage: 'load' }),\n    );\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    expect(fn).toHaveBeenCalledTimes(4); // 1 + 3 retries\n    expect(outcome.status).toBe('quarantined');\n    expect(outcome.errorClass).toBe('transient');\n    expect(outcome.failureStage).toBe('load');\n  });\n\n  it('should quarantine after Postgres serialization conflict 40001', async () => {\n    const fn = vi.fn().mockRejectedValue(\n      Object.assign(new Error('could not serialize access'), { code: '40001', stage: 'load' }),\n    );\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    expect(fn).toHaveBeenCalledTimes(4);\n    expect(outcome.status).toBe('quarantined');\n    expect(outcome.errorClass).toBe('transient');\n    expect(outcome.errorCode).toBe('40001');\n  });\n\n  it('should still produce terminal outcome when report persistence fails', async () => {\n    // Simulate: the record itself succeeds, but a downstream operation\n    // (like report persistence) would fail. withTerminalOutcome only wraps\n    // the record processing, so the record outcome is still valid.\n    const fn = vi.fn().mockResolvedValue({\n      ...base,\n      status: 'loaded' as const,\n      action: 'create' as const,\n      targetId: 'A1',\n    });\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    expect(outcome.status).toBe('loaded');\n    expect(outcome.targetId).toBe('A1');\n  });\n\n  it('should hold terminal state invariant under mixed fault injection', { timeout: 30_000 }, async () => {\n    const inputs = Array.from({ length: 20 }, (_, i) => ({\n      entityType: 'contacts',\n      legacyId: `L${i}`,\n    }));\n\n    const outcomes: RecordOutcome[] = [];\n\n    for (const input of inputs) {\n      const idx = Number(input.legacyId.slice(1));\n      const outcome = await withTerminalOutcome(input, async () => {\n        // Inject various faults\n        if (idx % 5 === 0) throw Object.assign(new Error('timeout'), { code: '57014' });\n        if (idx % 7 === 0) throw new Error('Validation failed');\n        if (idx % 11 === 0) throw null; // weird throw\n        return { ...input, status: 'loaded' as const, action: 'create' as const };\n      });\n      outcomes.push(outcome);\n    }\n\n    // TERM-01: every input has exactly one outcome\n    expect(outcomes).toHaveLength(20);\n\n    // Every outcome has a valid terminal status\n    for (const o of outcomes) {\n      expect(['loaded', 'quarantined', 'manual_review', 'skipped']).toContain(o.status);\n    }\n\n    // Sum check\n    const loaded = outcomes.filter((o) => o.status === 'loaded').length;\n    const quarantined = outcomes.filter((o) => o.status === 'quarantined').length;\n    const manual = outcomes.filter((o) => o.status === 'manual_review').length;\n    const skipped = outcomes.filter((o) => o.status === 'skipped').length;\n    expect(loaded + quarantined + manual + skipped).toBe(20);\n  });\n\n  it('should classify ECONNREFUSED as transient', () => {\n    expect(classifyError(new Error('connect ECONNREFUSED 127.0.0.1:5432'))).toBe('transient');\n  });\n\n  it('should classify Postgres deadlock 40P01 as transient', () => {\n    expect(classifyError({ code: '40P01' })).toBe('transient');\n  });\n\n  it('should classify Postgres connection terminated 08006 as transient', () => {\n    expect(classifyError({ code: '08006' })).toBe('transient');\n  });\n\n  it('should classify unique violation 23505 as permanent', () => {\n    expect(classifyError({ code: '23505' })).toBe('permanent');\n  });\n\n  it('should classify check constraint 23514 as permanent', () => {\n    expect(classifyError({ code: '23514' })).toBe('permanent');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\OPS-01-quarantine.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\OPS-01-quarantine.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect, vi } from 'vitest';\n\nimport {\n  withTerminalOutcome,\n  classifyError,\n} from '../../pipeline/with-terminal-outcome.js';\n\ndescribe('OPS-01: Retry and quarantine determinism', () => {\n  const base = { entityType: 'contacts', legacyId: 'L1' };\n\n  it('should retry transient errors up to MAX_RETRIES then quarantine', async () => {\n    const fn = vi.fn().mockRejectedValue(\n      Object.assign(new Error('serialization_failure'), { code: '40001' }),\n    );\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    // 1 initial + 3 retries = 4 calls\n    expect(fn).toHaveBeenCalledTimes(4);\n    expect(outcome.status).toBe('quarantined');\n    expect(outcome.errorClass).toBe('transient');\n  });\n\n  it('should quarantine permanent errors immediately (no retry)', async () => {\n    const fn = vi.fn().mockRejectedValue(\n      new Error('Validation failed: email is required'),\n    );\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    expect(fn).toHaveBeenCalledTimes(1);\n    expect(outcome.status).toBe('quarantined');\n    expect(outcome.errorClass).toBe('permanent');\n  });\n\n  it('should succeed on retry after transient failure', async () => {\n    let attempt = 0;\n    const fn = vi.fn().mockImplementation(async () => {\n      attempt++;\n      if (attempt <= 2) {\n        throw Object.assign(new Error('timeout'), { code: '57014' });\n      }\n      return { ...base, status: 'loaded', action: 'create', targetId: 'A1' };\n    });\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    expect(fn).toHaveBeenCalledTimes(3);\n    expect(outcome.status).toBe('loaded');\n    expect(outcome.targetId).toBe('A1');\n  });\n\n  it('should classify Postgres serialization failure as transient', () => {\n    expect(classifyError({ code: '40001' })).toBe('transient');\n  });\n\n  it('should classify Postgres deadlock as transient', () => {\n    expect(classifyError({ code: '40P01' })).toBe('transient');\n  });\n\n  it('should classify connection reset as transient', () => {\n    expect(classifyError(new Error('ECONNRESET'))).toBe('transient');\n  });\n\n  it('should classify timeout string as transient', () => {\n    expect(classifyError(new Error('Request timeout after 30s'))).toBe('transient');\n  });\n\n  it('should classify unknown errors as permanent', () => {\n    expect(classifyError(new Error('Validation failed'))).toBe('permanent');\n    expect(classifyError(new Error('Unique constraint violation'))).toBe('permanent');\n    expect(classifyError({ code: '23505' })).toBe('permanent');\n  });\n\n  it('should never throw from withTerminalOutcome', async () => {\n    // Even if fn throws something weird\n    const fn = vi.fn().mockRejectedValue(null);\n\n    const outcome = await withTerminalOutcome(base, fn);\n    expect(outcome.status).toBe('quarantined');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\OPS-02-checkpoints.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\OPS-02-checkpoints.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { PerfTracker } from '../../pipeline/perf-tracker.js';\nimport type { StepCheckpoint } from '../../types/checkpoint.js';\n\ndescribe('OPS-02: Step-level checkpoint resume precision', () => {\n  it('should represent a valid checkpoint shape', () => {\n    const checkpoint: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 500 },\n      batchIndex: 3,\n      loadedUpTo: 327,\n      transformVersion: 'abc123',\n    };\n\n    expect(checkpoint.batchIndex).toBe(3);\n    expect(checkpoint.loadedUpTo).toBe(327);\n    expect(checkpoint.cursor).toEqual({ type: 'offset', offset: 500 });\n  });\n\n  it('should allow optional planFingerprint', () => {\n    const checkpoint: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 0 },\n      batchIndex: 0,\n      loadedUpTo: 0,\n      transformVersion: 'v1',\n      planFingerprint: 'fp-abc',\n    };\n\n    expect(checkpoint.planFingerprint).toBe('fp-abc');\n  });\n\n  it('should resume from loadedUpTo after simulated crash', () => {\n    // Simulate: crash at index 327 out of 1000\n    const checkpoint: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 500 },\n      batchIndex: 2,\n      loadedUpTo: 327,\n      transformVersion: 'v1',\n    };\n\n    // Resume logic: start processing at index loadedUpTo + 1\n    const resumeFrom = checkpoint.loadedUpTo + 1;\n    expect(resumeFrom).toBe(328);\n\n    // Verify records 0..327 are skipped\n    const totalRecords = 1000;\n    const recordsToProcess = totalRecords - resumeFrom;\n    expect(recordsToProcess).toBe(672);\n  });\n\n  it('should advance cursor on batch completion', () => {\n    const before: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 500 },\n      batchIndex: 2,\n      loadedUpTo: 499,\n      transformVersion: 'v1',\n    };\n\n    // After batch completes: advance cursor, reset loadedUpTo, increment batchIndex\n    const after: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 1000 },\n      batchIndex: before.batchIndex + 1,\n      loadedUpTo: 0,\n      transformVersion: before.transformVersion,\n    };\n\n    expect(after.batchIndex).toBe(3);\n    expect(after.loadedUpTo).toBe(0);\n    expect(after.cursor).toEqual({ type: 'offset', offset: 1000 });\n  });\n\n  it('should support all cursor types', () => {\n    const offsetCheckpoint: StepCheckpoint = {\n      cursor: { type: 'offset', offset: 100 },\n      batchIndex: 0,\n      loadedUpTo: 0,\n      transformVersion: 'v1',\n    };\n    expect(offsetCheckpoint.cursor?.type).toBe('offset');\n\n    const idCheckpoint: StepCheckpoint = {\n      cursor: { type: 'id', lastId: 'uuid-abc' },\n      batchIndex: 0,\n      loadedUpTo: 0,\n      transformVersion: 'v1',\n    };\n    expect(idCheckpoint.cursor?.type).toBe('id');\n\n    const nullCheckpoint: StepCheckpoint = {\n      cursor: null,\n      batchIndex: 0,\n      loadedUpTo: 0,\n      transformVersion: 'v1',\n    };\n    expect(nullCheckpoint.cursor).toBeNull();\n  });\n});\n\ndescribe('OPS-02: PerfTracker correctness', () => {\n  it('should track p50 and p95 latencies', () => {\n    const tracker = new PerfTracker();\n\n    // Simulate 100 measurements\n    for (let i = 1; i <= 100; i++) {\n      const end = tracker.start('test_op');\n      // Immediately end (near-zero latency)\n      end();\n    }\n\n    expect(tracker.count('test_op')).toBe(100);\n    expect(tracker.p50('test_op')).toBeGreaterThanOrEqual(0);\n    expect(tracker.p95('test_op')).toBeGreaterThanOrEqual(0);\n  });\n\n  it('should return 0 for unknown labels', () => {\n    const tracker = new PerfTracker();\n    expect(tracker.p50('unknown')).toBe(0);\n    expect(tracker.p95('unknown')).toBe(0);\n    expect(tracker.count('unknown')).toBe(0);\n  });\n\n  it('should produce a report with all tracked labels', () => {\n    const tracker = new PerfTracker();\n\n    const end1 = tracker.start('extract');\n    end1();\n    const end2 = tracker.start('detect');\n    end2();\n\n    const report = tracker.toReport();\n    expect(Object.keys(report)).toContain('extract');\n    expect(Object.keys(report)).toContain('detect');\n    expect(report['extract']!.count).toBe(1);\n    expect(report['detect']!.count).toBe(1);\n  });\n\n  it('should reset all timings', () => {\n    const tracker = new PerfTracker();\n    const end = tracker.start('op');\n    end();\n    expect(tracker.count('op')).toBe(1);\n\n    tracker.reset();\n    expect(tracker.count('op')).toBe(0);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\P1-conflict-thresholds.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\P1-conflict-thresholds.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { DEFAULT_CONFLICT_THRESHOLDS } from '../../types/conflict-thresholds.js';\n\nimport type { ConflictThresholds } from '../../types/conflict-thresholds.js';\n\ndescribe('ACC-04: Configurable Conflict Thresholds', () => {\n  it('should have sensible defaults', () => {\n    expect(DEFAULT_CONFLICT_THRESHOLDS.autoMerge).toBe(60);\n    expect(DEFAULT_CONFLICT_THRESHOLDS.manualReview).toBe(30);\n    expect(DEFAULT_CONFLICT_THRESHOLDS.autoMerge).toBeGreaterThan(DEFAULT_CONFLICT_THRESHOLDS.manualReview);\n  });\n\n  it('should allow custom thresholds', () => {\n    const custom: ConflictThresholds = { autoMerge: 80, manualReview: 50 };\n    expect(custom.autoMerge).toBe(80);\n    expect(custom.manualReview).toBe(50);\n  });\n\n  it('should route scores correctly based on thresholds', () => {\n    const thresholds: ConflictThresholds = { autoMerge: 60, manualReview: 30 };\n\n    const decide = (score: number): 'merge' | 'manual_review' | 'create' => {\n      if (score >= thresholds.autoMerge) return 'merge';\n      if (score >= thresholds.manualReview) return 'manual_review';\n      return 'create';\n    };\n\n    expect(decide(80)).toBe('merge');\n    expect(decide(60)).toBe('merge');\n    expect(decide(59)).toBe('manual_review');\n    expect(decide(30)).toBe('manual_review');\n    expect(decide(29)).toBe('create');\n    expect(decide(0)).toBe('create');\n  });\n\n  it('should support strict thresholds (high autoMerge)', () => {\n    const strict: ConflictThresholds = { autoMerge: 90, manualReview: 60 };\n\n    const decide = (score: number): string => {\n      if (score >= strict.autoMerge) return 'merge';\n      if (score >= strict.manualReview) return 'manual_review';\n      return 'create';\n    };\n\n    expect(decide(95)).toBe('merge');\n    expect(decide(70)).toBe('manual_review');\n    expect(decide(50)).toBe('create');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\P1-readiness-gate.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\P1-readiness-gate.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { ReadinessGate, MappingCompletenessCheck, MatchKeyNullRateCheck } from '../../gates/readiness-gate.js';\n\nimport type { MigrationJob } from '../../types/migration-job.js';\n\nconst makeJob = (overrides?: Partial<MigrationJob>): MigrationJob => ({\n  id: 'job-1',\n  orgId: 'org-1',\n  entityType: 'contacts',\n  sourceConfig: { transport: 'csv', systemName: 'legacy', filePath: '/tmp/test.csv' },\n  fieldMappings: [\n    { sourceField: 'name', targetField: 'name' },\n    { sourceField: 'email', targetField: 'email' },\n  ],\n  mergePolicies: [],\n  conflictStrategy: 'merge',\n  status: 'pending',\n  checkpointCursor: null,\n  ...overrides,\n});\n\ndescribe('OPS-03: Preflight Readiness Gate', () => {\n  it('should pass with no checks registered', async () => {\n    const gate = new ReadinessGate();\n    const result = await gate.check(makeJob());\n    expect(result.passed).toBe(true);\n  });\n\n  it('should pass when all checks pass', async () => {\n    const gate = new ReadinessGate();\n    gate.addCheck(new MappingCompletenessCheck());\n    const result = await gate.check(makeJob());\n    expect(result.passed).toBe(true);\n  });\n\n  it('should fail when mapping completeness check fails', async () => {\n    const gate = new ReadinessGate();\n    gate.addCheck(new MappingCompletenessCheck());\n    const result = await gate.check(makeJob({ fieldMappings: [] }));\n    expect(result.passed).toBe(false);\n    expect(result.reason).toContain('mapping-completeness');\n  });\n\n  it('should produce a structured readiness report', async () => {\n    const gate = new ReadinessGate();\n    gate.addCheck(new MappingCompletenessCheck());\n    await gate.check(makeJob());\n    const report = gate.getLastReport();\n    expect(report).not.toBeNull();\n    expect(report!.jobId).toBe('job-1');\n    expect(report!.entityType).toBe('contacts');\n    expect(report!.overallPassed).toBe(true);\n    expect(report!.checks).toHaveLength(1);\n    expect(report!.checks[0]!.name).toBe('mapping-completeness');\n  });\n\n  it('should warn on high null rate match keys', async () => {\n    const gate = new ReadinessGate();\n    gate.addCheck(new MatchKeyNullRateCheck({\n      threshold: 0.3,\n      sampleFn: async () => ({ email: 0.5, phone: 0.1 }),\n    }));\n    const result = await gate.check(makeJob());\n    // Warning doesn't fail by default\n    expect(result.passed).toBe(true);\n    const report = gate.getLastReport();\n    expect(report!.warningCount).toBe(1);\n  });\n\n  it('should fail on warnings when failOnWarnings is true', async () => {\n    const gate = new ReadinessGate({ failOnWarnings: true });\n    gate.addCheck(new MatchKeyNullRateCheck({\n      threshold: 0.3,\n      sampleFn: async () => ({ email: 0.5 }),\n    }));\n    const result = await gate.check(makeJob());\n    expect(result.passed).toBe(false);\n    expect(result.reason).toContain('match-key-null-rate');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\P2-kpi-tracker.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\P2-kpi-tracker.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\n\nimport { KpiTracker } from '../../pipeline/kpi-tracker.js';\n\ndescribe('P2-2: Accuracy KPI Tracker', () => {\n  it('should track counter increments', () => {\n    const kpi = new KpiTracker();\n    kpi.increment('duplicates_prevented');\n    kpi.increment('duplicates_prevented');\n    kpi.increment('auto_merge_count', 5);\n    expect(kpi.get('duplicates_prevented')).toBe(2);\n    expect(kpi.get('auto_merge_count')).toBe(5);\n    expect(kpi.get('nonexistent')).toBe(0);\n  });\n\n  it('should detect cursor replays', () => {\n    const kpi = new KpiTracker();\n    kpi.trackLegacyId('legacy-1');\n    kpi.trackLegacyId('legacy-2');\n    kpi.trackLegacyId('legacy-1'); // replay\n    kpi.trackLegacyId('legacy-3');\n    kpi.trackLegacyId('legacy-2'); // replay\n    expect(kpi.get('cursor_replays_detected')).toBe(2);\n  });\n\n  it('should produce a structured report', () => {\n    const kpi = new KpiTracker();\n    kpi.increment('duplicates_prevented', 3);\n    kpi.increment('manual_review_count', 7);\n    kpi.trackLegacyId('a');\n    kpi.trackLegacyId('b');\n    kpi.trackLegacyId('a');\n\n    const report = kpi.toReport();\n    expect(report['duplicates_prevented']).toBe(3);\n    expect(report['manual_review_count']).toBe(7);\n    expect(report['cursor_replays_detected']).toBe(1);\n    expect(report['unique_legacy_ids_seen']).toBe(2);\n  });\n\n  it('should start with zero counters', () => {\n    const kpi = new KpiTracker();\n    const report = kpi.toReport();\n    expect(report['unique_legacy_ids_seen']).toBe(0);\n    expect(Object.keys(report)).toHaveLength(1); // only unique_legacy_ids_seen\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\TERM-01-terminal-state.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\TERM-01-terminal-state.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect, vi } from 'vitest';\n\nimport { withTerminalOutcome } from '../../pipeline/with-terminal-outcome.js';\nimport type { RecordOutcome } from '../../types/record-outcome.js';\n\ndescribe('TERM-01: Every record reaches exactly one terminal state', () => {\n  const base = { entityType: 'contacts', legacyId: 'L1' };\n\n  it('should return loaded outcome on success', async () => {\n    const outcome = await withTerminalOutcome(base, async () => ({\n      ...base,\n      status: 'loaded' as const,\n      action: 'create' as const,\n      targetId: 'A1',\n    }));\n\n    expect(outcome.status).toBe('loaded');\n    expect(outcome.targetId).toBe('A1');\n  });\n\n  it('should return quarantined outcome on permanent error', async () => {\n    const outcome = await withTerminalOutcome(base, async () => {\n      throw new Error('Validation failed');\n    });\n\n    expect(outcome.status).toBe('quarantined');\n    expect(outcome.errorClass).toBe('permanent');\n    expect(outcome.entityType).toBe('contacts');\n    expect(outcome.legacyId).toBe('L1');\n  });\n\n  it('should produce one RecordOutcome per input (mixed batch)', async () => {\n    const inputs = [\n      { entityType: 'contacts', legacyId: 'L1' },\n      { entityType: 'contacts', legacyId: 'L2' },\n      { entityType: 'contacts', legacyId: 'L3' },\n      { entityType: 'contacts', legacyId: 'L4' },\n    ];\n\n    const outcomes: RecordOutcome[] = [];\n\n    for (const input of inputs) {\n      const outcome = await withTerminalOutcome(input, async () => {\n        if (input.legacyId === 'L3') throw new Error('bad row');\n        return { ...input, status: 'loaded' as const, action: 'create' as const };\n      });\n      outcomes.push(outcome);\n    }\n\n    expect(outcomes).toHaveLength(4);\n    expect(outcomes.filter((o) => o.status === 'loaded')).toHaveLength(3);\n    expect(outcomes.filter((o) => o.status === 'quarantined')).toHaveLength(1);\n  });\n\n  it('should satisfy processed = loaded + quarantined + manual_review + skipped', () => {\n    const outcomes: RecordOutcome[] = [\n      { entityType: 'contacts', legacyId: 'L1', status: 'loaded', action: 'create' },\n      { entityType: 'contacts', legacyId: 'L2', status: 'loaded', action: 'update' },\n      { entityType: 'contacts', legacyId: 'L3', status: 'quarantined', errorClass: 'permanent' },\n      { entityType: 'contacts', legacyId: 'L4', status: 'skipped', action: 'skip' },\n      { entityType: 'contacts', legacyId: 'L5', status: 'manual_review' },\n    ];\n\n    const loaded = outcomes.filter((o) => o.status === 'loaded').length;\n    const quarantined = outcomes.filter((o) => o.status === 'quarantined').length;\n    const manualReview = outcomes.filter((o) => o.status === 'manual_review').length;\n    const skipped = outcomes.filter((o) => o.status === 'skipped').length;\n\n    expect(loaded + quarantined + manualReview + skipped).toBe(outcomes.length);\n  });\n\n  it('should never have a record without a terminal status', async () => {\n    const fn = vi.fn().mockRejectedValue(new Error('crash'));\n\n    const outcome = await withTerminalOutcome(base, fn);\n\n    // withTerminalOutcome NEVER throws ÔÇö always returns an outcome\n    expect(outcome).toBeDefined();\n    expect(outcome.status).toBeDefined();\n    expect(['loaded', 'quarantined', 'manual_review', 'skipped']).toContain(outcome.status);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\aud-06-canonical-json.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\aud-06-canonical-json.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { canonicalStringify, hashCanonical } from '../../audit/canonical-json.js';\nimport { buildSignedReport } from '../../audit/signed-report.js';\nimport type { ReportInputs } from '../../audit/signed-report.js';\n\n/**\n * AUD-06: Signed report uses canonical JSON serialization.\n *\n * - Key order doesn't affect hash\n * - Arrays preserve order\n * - Undefined values stripped\n * - Report includes all fingerprints\n */\ndescribe('AUD-06: Canonical JSON hashing', () => {\n  it('should produce stable hashes regardless of key order', () => {\n    const obj1 = { b: 2, a: 1, c: [3, 2, 1] };\n    const obj2 = { a: 1, b: 2, c: [3, 2, 1] };\n\n    const hash1 = hashCanonical(obj1);\n    const hash2 = hashCanonical(obj2);\n\n    expect(hash1).toBe(hash2);\n  });\n\n  it('should preserve array order', () => {\n    const obj1 = { items: [1, 2, 3] };\n    const obj2 = { items: [3, 2, 1] };\n\n    const hash1 = hashCanonical(obj1);\n    const hash2 = hashCanonical(obj2);\n\n    expect(hash1).not.toBe(hash2);\n  });\n\n  it('should strip undefined values', () => {\n    const obj1 = { a: 1, b: undefined };\n    const obj2 = { a: 1 };\n\n    const str1 = canonicalStringify(obj1);\n    const str2 = canonicalStringify(obj2);\n\n    expect(str1).toBe(str2);\n  });\n\n  it('should handle nested objects with deep key sorting', () => {\n    const obj1 = { z: { b: 2, a: 1 }, a: { d: 4, c: 3 } };\n    const obj2 = { a: { c: 3, d: 4 }, z: { a: 1, b: 2 } };\n\n    expect(hashCanonical(obj1)).toBe(hashCanonical(obj2));\n  });\n\n  it('should produce deterministic SHA-256 hashes', () => {\n    const obj = { test: 'value', num: 42 };\n    const hash1 = hashCanonical(obj);\n    const hash2 = hashCanonical(obj);\n\n    expect(hash1).toBe(hash2);\n    expect(hash1).toHaveLength(64); // SHA-256 hex = 64 chars\n  });\n\n  it('should include all fingerprints in signed report', () => {\n    const inputs: ReportInputs = {\n      job: {\n        id: 'job-1',\n        orgId: 'org-1',\n        entityType: 'contacts',\n        sourceConfig: { transport: 'csv', systemName: 'legacy', filePath: '' },\n        fieldMappings: [{ sourceField: 'Name', targetField: 'name' }],\n        mergePolicies: [],\n        conflictStrategy: 'skip',\n        status: 'completed',\n        checkpointCursor: null,\n        recordsSuccess: 10,\n        recordsFailed: 0,\n      },\n      result: {\n        recordsProcessed: 10,\n        recordsCreated: 8,\n        recordsUpdated: 2,\n        recordsMerged: 0,\n        recordsSkipped: 0,\n        recordsFailed: 0,\n        recordsManualReview: 0,\n      },\n      sourceSchemaFingerprint: 'abc123',\n      transformSteps: [\n        { name: 'trim_whitespace', order: 10 },\n        { name: 'type_coercion', order: 100 },\n      ],\n      conflictDetectorName: 'contacts_email_phone',\n      conflictDetectorMatchKeys: ['email', 'phone'],\n      mergeEvidenceIds: [],\n      manualReviewIds: [],\n    };\n\n    const report = buildSignedReport(inputs);\n\n    expect(report.sourceSchemaFingerprint).toBe('abc123');\n    expect(report.mappingFingerprint).toBeDefined();\n    expect(report.transformChainFingerprint).toBeDefined();\n    expect(report.strategyFingerprint).toBeDefined();\n    expect(report.reportHash).toBeDefined();\n    expect(report.reportHash).toHaveLength(64);\n\n    // Verify determinism\n    const report2 = buildSignedReport(inputs);\n    expect(report.mappingFingerprint).toBe(report2.mappingFingerprint);\n    expect(report.transformChainFingerprint).toBe(report2.transformChainFingerprint);\n    expect(report.strategyFingerprint).toBe(report2.strategyFingerprint);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\csv-adapter.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\csv-adapter.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { CsvLegacyAdapter } from '../../adapters/csv-adapter.js';\n\ndescribe('CSV Legacy Adapter', () => {\n  const rows = [\n    { id: '1', name: 'Alice', email: 'alice@example.com' },\n    { id: '2', name: 'Bob', email: 'bob@example.com' },\n    { id: '3', name: 'Charlie', email: 'charlie@example.com' },\n    { id: '4', name: 'Diana', email: 'diana@example.com' },\n    { id: '5', name: 'Eve', email: 'eve@example.com' },\n  ];\n\n  function createAdapter() {\n    return new CsvLegacyAdapter({\n      systemName: 'test_csv',\n      entityType: 'contacts',\n      rows,\n      idColumn: 'id',\n    });\n  }\n\n  it('should extract first batch with offset cursor', async () => {\n    const adapter = createAdapter();\n    const batch = await adapter.extractBatch('contacts', 2, null);\n\n    expect(batch.records).toHaveLength(2);\n    expect(batch.records[0]!.legacyId).toBe('1');\n    expect(batch.records[1]!.legacyId).toBe('2');\n    expect(batch.nextCursor).toEqual({ type: 'offset', offset: 2 });\n  });\n\n  it('should extract subsequent batch using cursor', async () => {\n    const adapter = createAdapter();\n    const batch = await adapter.extractBatch('contacts', 2, { type: 'offset', offset: 2 });\n\n    expect(batch.records).toHaveLength(2);\n    expect(batch.records[0]!.legacyId).toBe('3');\n    expect(batch.records[1]!.legacyId).toBe('4');\n    expect(batch.nextCursor).toEqual({ type: 'offset', offset: 4 });\n  });\n\n  it('should return null cursor on last batch', async () => {\n    const adapter = createAdapter();\n    const batch = await adapter.extractBatch('contacts', 2, { type: 'offset', offset: 4 });\n\n    expect(batch.records).toHaveLength(1);\n    expect(batch.records[0]!.legacyId).toBe('5');\n    expect(batch.nextCursor).toBeNull();\n  });\n\n  it('should return empty batch when offset exceeds rows', async () => {\n    const adapter = createAdapter();\n    const batch = await adapter.extractBatch('contacts', 2, { type: 'offset', offset: 10 });\n\n    expect(batch.records).toHaveLength(0);\n    expect(batch.nextCursor).toBeNull();\n  });\n\n  it('should throw for wrong entity type', async () => {\n    const adapter = createAdapter();\n    await expect(\n      adapter.extractBatch('invoices', 2, null)\n    ).rejects.toThrow(\"CsvLegacyAdapter configured for 'contacts', got 'invoices'\");\n  });\n\n  it('should introspect schema from first row', async () => {\n    const adapter = createAdapter();\n    const schema = await adapter.getSchema('contacts');\n\n    expect(schema.tableName).toBe('csv_import');\n    expect(schema.columns).toHaveLength(3);\n    expect(schema.columns.map((c) => c.name)).toEqual(['id', 'name', 'email']);\n  });\n\n  it('should return empty schema for empty rows', async () => {\n    const adapter = new CsvLegacyAdapter({\n      systemName: 'empty',\n      entityType: 'contacts',\n      rows: [],\n      idColumn: 'id',\n    });\n    const schema = await adapter.getSchema('contacts');\n    expect(schema.columns).toHaveLength(0);\n  });\n\n  it('should report healthy when rows exist', async () => {\n    const adapter = createAdapter();\n    expect(await adapter.healthCheck()).toBe(true);\n  });\n\n  it('should report unhealthy when no rows', async () => {\n    const adapter = new CsvLegacyAdapter({\n      systemName: 'empty',\n      entityType: 'contacts',\n      rows: [],\n      idColumn: 'id',\n    });\n    expect(await adapter.healthCheck()).toBe(false);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\det-03-registry-total.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\det-03-registry-total.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport {\n  CONFLICT_DETECTOR_REGISTRY,\n  getConflictDetector,\n} from '../../strategies/conflict-detector.js';\n\n/**\n * DET-03: ConflictDetector registry is total.\n *\n * - Every registered entity type has a detector\n * - detector.entityType matches the registry key\n * - matchKeys is always an array\n * - getConflictDetector throws for unknown entity types\n */\ndescribe('DET-03: ConflictDetector registry is total', () => {\n  it('should have detector for all registered entity types', () => {\n    const registeredTypes = Object.keys(CONFLICT_DETECTOR_REGISTRY);\n\n    expect(registeredTypes.length).toBeGreaterThan(0);\n\n    for (const entityType of registeredTypes) {\n      const detector = CONFLICT_DETECTOR_REGISTRY[entityType]!;\n\n      expect(detector).toBeDefined();\n      expect(detector.entityType).toBe(entityType);\n      expect(detector.name).toBeDefined();\n      expect(Array.isArray(detector.matchKeys)).toBe(true);\n    }\n  });\n\n  it('should throw for unknown entity types', () => {\n    expect(() => {\n      getConflictDetector('unknown_entity_xyz');\n    }).toThrow('No conflict detector for entity type: unknown_entity_xyz');\n  });\n\n  it('should detect entity type mismatch', () => {\n    // Manually verify the mismatch check in getConflictDetector\n    // by temporarily creating a bad entry\n    const original = CONFLICT_DETECTOR_REGISTRY['contacts']!;\n    expect(original.entityType).toBe('contacts');\n\n    // getConflictDetector should pass for correct mapping\n    const detector = getConflictDetector('contacts');\n    expect(detector.entityType).toBe('contacts');\n  });\n\n  it('should have matchKeys for detectors that do conflict detection', () => {\n    const contacts = getConflictDetector('contacts');\n    expect(contacts.matchKeys.length).toBeGreaterThan(0);\n    expect(contacts.matchKeys).toContain('email');\n    expect(contacts.matchKeys).toContain('phone');\n\n    const invoices = getConflictDetector('invoices');\n    expect(invoices.matchKeys.length).toBeGreaterThan(0);\n    expect(invoices.matchKeys).toContain('invoiceNumber');\n  });\n\n  it('should have empty matchKeys for NoConflictDetector', () => {\n    const companies = getConflictDetector('companies');\n    expect(companies.matchKeys).toHaveLength(0);\n    expect(companies.name).toBe('no_conflict');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\det-05-transform-order.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\det-05-transform-order.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport {\n  TransformChain,\n  TrimWhitespaceStep,\n  NormalizeWhitespaceStep,\n  PhoneNormalizeStep,\n  EmailNormalizeStep,\n  TypeCoercionStep,\n  buildStandardTransformChain,\n} from '../../transforms/transform-chain.js';\n\n/**\n * DET-05: Transform steps execute in monotonic order, coercion last.\n */\ndescribe('DET-05: Transform steps execute in monotonic order', () => {\n  it('should sort steps by order on addStep', () => {\n    const chain = new TransformChain();\n\n    chain.addStep(new TypeCoercionStep());     // order: 100\n    chain.addStep(new TrimWhitespaceStep());   // order: 10\n    chain.addStep(new PhoneNormalizeStep());    // order: 30\n\n    const steps = chain.getSteps();\n\n    expect(steps[0]!.order).toBe(10);\n    expect(steps[1]!.order).toBe(30);\n    expect(steps[2]!.order).toBe(100);\n  });\n\n  it('should always run coercion last in standard chain', () => {\n    const chain = buildStandardTransformChain();\n    const steps = chain.getSteps();\n\n    const lastStep = steps[steps.length - 1]!;\n    expect(lastStep.name).toBe('type_coercion');\n    expect(lastStep.order).toBe(100);\n  });\n\n  it('should maintain order even when steps added out of order', () => {\n    const chain = new TransformChain();\n\n    chain.addStep(new TypeCoercionStep());         // 100\n    chain.addStep(new EmailNormalizeStep());        // 40\n    chain.addStep(new TrimWhitespaceStep());        // 10\n    chain.addStep(new NormalizeWhitespaceStep());   // 20\n    chain.addStep(new PhoneNormalizeStep());         // 30\n\n    const steps = chain.getSteps();\n    const orders = steps.map((s) => s.order);\n\n    expect(orders).toEqual([10, 20, 30, 40, 100]);\n  });\n\n  it('should apply transforms in order', async () => {\n    const chain = buildStandardTransformChain();\n\n    // Email with leading/trailing whitespace\n    const result = await chain.transform(\n      '  Test@Example.COM  ',\n      'email',\n      'email',\n      { entityType: 'contacts', orgId: 'org-1' }\n    );\n\n    // TrimWhitespace(10) ÔåÆ 'Test@Example.COM'\n    // EmailNormalize(40) ÔåÆ 'test@example.com'\n    expect(result).toBe('test@example.com');\n  });\n\n  it('should normalize phone numbers', async () => {\n    const chain = buildStandardTransformChain();\n\n    const result = await chain.transform(\n      ' +60 (12) 345-6789 ',\n      'phone',\n      'phone',\n      { entityType: 'contacts', orgId: 'org-1' }\n    );\n\n    // TrimWhitespace(10) ÔåÆ '+60 (12) 345-6789'\n    // PhoneNormalize(30) ÔåÆ '+60123456789'\n    expect(result).toBe('+60123456789');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\rate-limiter.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\rate-limiter.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { RateLimiter } from '../../worker/rate-limiter.js';\n\ndescribe('RateLimiter', () => {\n  it('should allow immediate acquisition within burst', async () => {\n    const limiter = new RateLimiter(100, 100);\n    const start = Date.now();\n    await limiter.acquire(50);\n    const elapsed = Date.now() - start;\n\n    // Should be near-instant (within burst capacity)\n    expect(elapsed).toBeLessThan(50);\n  });\n\n  it('should throttle when burst is exhausted', async () => {\n    const limiter = new RateLimiter(100, 10);\n\n    // Exhaust burst\n    await limiter.acquire(10);\n\n    // Next acquire should wait\n    const start = Date.now();\n    await limiter.acquire(5);\n    const elapsed = Date.now() - start;\n\n    // Should have waited ~50ms for 5 tokens at 100/s\n    expect(elapsed).toBeGreaterThanOrEqual(30);\n  });\n\n  it('should refill tokens over time', async () => {\n    const limiter = new RateLimiter(1000, 10);\n\n    // Exhaust burst\n    await limiter.acquire(10);\n\n    // Wait for refill\n    await new Promise((resolve) => setTimeout(resolve, 50));\n\n    // Should have refilled ~50 tokens (1000/s * 0.05s)\n    const start = Date.now();\n    await limiter.acquire(10);\n    const elapsed = Date.now() - start;\n\n    // Should be near-instant since we waited for refill\n    expect(elapsed).toBeLessThan(50);\n  });\n\n  it('should not exceed max burst', async () => {\n    const limiter = new RateLimiter(10000, 5);\n\n    // Wait for potential over-refill\n    await new Promise((resolve) => setTimeout(resolve, 50));\n\n    // Should still only have maxBurst (5) tokens\n    // Acquiring 6 should require waiting\n    const start = Date.now();\n    await limiter.acquire(5);\n    const elapsed = Date.now() - start;\n\n    expect(elapsed).toBeLessThan(50); // Within burst\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\res-01-no-zombies.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\res-01-no-zombies.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect, vi } from 'vitest';\nimport type { PipelineDb } from '../../pipeline/pipeline-base.js';\nimport { MigrationPipelineBase } from '../../pipeline/pipeline-base.js';\nimport type {\n  UpsertPlan,\n  LoadResult,\n  BatchResult,\n  LegacyRecord,\n  TransformedRecord,\n  GateResult,\n} from '../../types/index.js';\n\n/**\n * RES-01: No zombie reservations.\n *\n * - reserveLineage inserts with state='reserved'\n * - if insert conflicts, attempts reclaim of stale reservation\n * - commitLineage enforces state transition reserved ÔåÆ committed\n * - double-commit throws\n * - failed create deletes reservation by lineageId only (D0.2)\n */\n\n// Minimal concrete pipeline for testing\nclass TestPipeline extends MigrationPipelineBase {\n  protected async planUpserts(): Promise<UpsertPlan> {\n    return { jobId: '', entityType: '', actions: [] };\n  }\n  protected async loadPlan(): Promise<LoadResult> {\n    return { created: [], updated: [], skipped: [], failed: [] };\n  }\n  protected async extractBatch(): Promise<BatchResult> {\n    return { records: [], nextCursor: null };\n  }\n  protected async transformBatch(records: LegacyRecord[]): Promise<TransformedRecord[]> {\n    return records;\n  }\n  protected async runPreflightGates(): Promise<GateResult> {\n    return { passed: true };\n  }\n  protected async runPostflightGates(): Promise<GateResult> {\n    return { passed: true };\n  }\n\n  // Expose protected methods for testing\n  public testReserveLineage(\n    jobId: string,\n    entityType: string,\n    legacyKey: { legacySystem: string; legacyId: string }\n  ) {\n    return this.reserveLineage(jobId, entityType, legacyKey);\n  }\n\n  public testCommitLineage(lineageId: string, afenaId: string) {\n    return this.commitLineage(lineageId, afenaId);\n  }\n}\n\nfunction makePipeline(db: PipelineDb): TestPipeline {\n  return new TestPipeline(\n    {\n      id: 'job-1',\n      orgId: 'org-1',\n      entityType: 'contacts',\n      sourceConfig: { transport: 'csv', systemName: 'legacy', filePath: '' },\n      fieldMappings: [],\n      mergePolicies: [],\n      conflictStrategy: 'skip',\n      status: 'running',\n      checkpointCursor: null,\n      recordsSuccess: 0,\n      recordsFailed: 0,\n    },\n    { orgId: 'org-1', workerId: 'worker-1', requestId: 'req-1' },\n    db\n  );\n}\n\ndescribe('RES-01: No zombie reservations', () => {\n  it('should win reservation when insert succeeds', async () => {\n    const db: PipelineDb = {\n      insertLineageReservation: vi.fn().mockResolvedValue({ id: 'lineage-1' }),\n      reclaimStaleReservation: vi.fn().mockResolvedValue(null),\n      commitLineage: vi.fn().mockResolvedValue(true),\n      deleteReservation: vi.fn().mockResolvedValue(undefined),\n    };\n\n    const pipeline = makePipeline(db);\n    const result = await pipeline.testReserveLineage('job-1', 'contacts', {\n      legacySystem: 'legacy',\n      legacyId: '12345',\n    });\n\n    expect(result.isWinner).toBe(true);\n    expect(result.lineageId).toBe('lineage-1');\n    expect(db.insertLineageReservation).toHaveBeenCalledOnce();\n  });\n\n  it('should attempt reclaim when insert conflicts', async () => {\n    const db: PipelineDb = {\n      insertLineageReservation: vi.fn().mockResolvedValue(null), // conflict\n      reclaimStaleReservation: vi.fn().mockResolvedValue({ id: 'reclaimed-1' }),\n      commitLineage: vi.fn().mockResolvedValue(true),\n      deleteReservation: vi.fn().mockResolvedValue(undefined),\n    };\n\n    const pipeline = makePipeline(db);\n    const result = await pipeline.testReserveLineage('job-1', 'contacts', {\n      legacySystem: 'legacy',\n      legacyId: '12345',\n    });\n\n    expect(result.isWinner).toBe(true);\n    expect(result.lineageId).toBe('reclaimed-1');\n    expect(db.reclaimStaleReservation).toHaveBeenCalledOnce();\n  });\n\n  it('should lose when insert conflicts and reclaim fails', async () => {\n    const db: PipelineDb = {\n      insertLineageReservation: vi.fn().mockResolvedValue(null),\n      reclaimStaleReservation: vi.fn().mockResolvedValue(null), // not stale\n      commitLineage: vi.fn().mockResolvedValue(true),\n      deleteReservation: vi.fn().mockResolvedValue(undefined),\n    };\n\n    const pipeline = makePipeline(db);\n    const result = await pipeline.testReserveLineage('job-1', 'contacts', {\n      legacySystem: 'legacy',\n      legacyId: '12345',\n    });\n\n    expect(result.isWinner).toBe(false);\n  });\n\n  it('should enforce state transition on commit', async () => {\n    const db: PipelineDb = {\n      insertLineageReservation: vi.fn().mockResolvedValue({ id: 'lineage-1' }),\n      reclaimStaleReservation: vi.fn().mockResolvedValue(null),\n      commitLineage: vi.fn().mockResolvedValue(true),\n      deleteReservation: vi.fn().mockResolvedValue(undefined),\n    };\n\n    const pipeline = makePipeline(db);\n    await pipeline.testCommitLineage('lineage-1', 'afena-uuid');\n\n    expect(db.commitLineage).toHaveBeenCalledWith('lineage-1', 'afena-uuid');\n  });\n\n  it('should throw on double-commit (not reserved)', async () => {\n    const db: PipelineDb = {\n      insertLineageReservation: vi.fn().mockResolvedValue({ id: 'lineage-1' }),\n      reclaimStaleReservation: vi.fn().mockResolvedValue(null),\n      commitLineage: vi.fn().mockResolvedValue(false), // already committed\n      deleteReservation: vi.fn().mockResolvedValue(undefined),\n    };\n\n    const pipeline = makePipeline(db);\n    await expect(\n      pipeline.testCommitLineage('lineage-1', 'afena-uuid')\n    ).rejects.toThrow('Lineage commit failed (not reserved): lineage-1');\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\snap-04-writable-only.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\snap-04-writable-only.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport {\n  CanonEntityWriteAdapter,\n  ENTITY_WRITABLE_CORE_FIELDS,\n  getEntityWriteAdapter,\n} from '../../adapters/entity-write-adapter.js';\n\n/**\n * SNAP-04: Snapshot contains only writable fields.\n *\n * - toWriteShape picks ONLY canonized writable fields\n * - System columns (id, org_id, created_at, version, etc.) are excluded\n * - custom_data is extracted separately\n * - All keys in core Ôèå ENTITY_WRITABLE_CORE_FIELDS[entityType]\n */\ndescribe('SNAP-04: Snapshot contains only writable fields', () => {\n  it('should capture only writable core fields', () => {\n    const rawRow = {\n      id: 'uuid',\n      org_id: 'test-org',\n      name: 'John Doe',\n      email: 'john@example.com',\n      phone: '+1234567890',\n      status: 'active',\n      notes: 'Some notes',\n      tags: ['tag1'],\n      created_at: new Date(),\n      updated_at: new Date(),\n      created_by: 'system',\n      updated_by: 'system',\n      version: 1,\n      is_deleted: false,\n      deleted_at: null,\n      deleted_by: null,\n      custom_data: { field1: 'value1' },\n    };\n\n    const adapter = getEntityWriteAdapter('contacts');\n    const { core, custom } = adapter.toWriteShape(rawRow);\n\n    // Should include only writable fields\n    expect(core).toHaveProperty('name');\n    expect(core).toHaveProperty('email');\n    expect(core).toHaveProperty('phone');\n    expect(core).toHaveProperty('status');\n\n    // Should NOT include system fields\n    expect(core).not.toHaveProperty('id');\n    expect(core).not.toHaveProperty('org_id');\n    expect(core).not.toHaveProperty('created_at');\n    expect(core).not.toHaveProperty('updated_at');\n    expect(core).not.toHaveProperty('created_by');\n    expect(core).not.toHaveProperty('updated_by');\n    expect(core).not.toHaveProperty('version');\n    expect(core).not.toHaveProperty('is_deleted');\n    expect(core).not.toHaveProperty('deleted_at');\n    expect(core).not.toHaveProperty('deleted_by');\n    expect(core).not.toHaveProperty('custom_data');\n\n    // Custom fields should be extracted\n    expect(custom).toEqual({ field1: 'value1' });\n  });\n\n  it('should match canonized writable field list exactly', () => {\n    const adapter = getEntityWriteAdapter('contacts');\n    const rawRow = {\n      name: 'Test',\n      email: 'test@example.com',\n      phone: '+1234567890',\n      status: 'active',\n      notes: 'Some notes',\n      tags: ['tag1'],\n      extra_field: 'should be ignored',\n    };\n\n    const { core } = adapter.toWriteShape(rawRow);\n\n    const writableFields = ENTITY_WRITABLE_CORE_FIELDS['contacts']!;\n\n    // All keys in core must be in ENTITY_WRITABLE_CORE_FIELDS\n    for (const key of Object.keys(core)) {\n      expect(writableFields).toContain(key);\n    }\n\n    // extra_field should NOT appear\n    expect(core).not.toHaveProperty('extra_field');\n  });\n\n  it('should handle missing custom_data gracefully', () => {\n    const adapter = getEntityWriteAdapter('contacts');\n    const rawRow = { name: 'Test' };\n\n    const { custom } = adapter.toWriteShape(rawRow);\n    expect(custom).toEqual({});\n  });\n\n  it('should throw for unknown entity types', () => {\n    expect(() => {\n      getEntityWriteAdapter('unknown_entity_xyz');\n    }).toThrow('No write adapter for entity type: unknown_entity_xyz');\n  });\n\n  it('should throw when writable fields not defined', () => {\n    const adapter = new CanonEntityWriteAdapter('nonexistent_entity');\n    expect(() => {\n      adapter.toWriteShape({ name: 'test' });\n    }).toThrow('No writable fields defined for entity type: nonexistent_entity');\n  });\n\n  it('should have writable fields defined for all registered adapters', () => {\n    const entityTypes = Object.keys(ENTITY_WRITABLE_CORE_FIELDS);\n\n    for (const entityType of entityTypes) {\n      const fields = ENTITY_WRITABLE_CORE_FIELDS[entityType]!;\n      expect(fields.length).toBeGreaterThan(0);\n\n      const adapter = getEntityWriteAdapter(entityType);\n      expect(adapter.entityType).toBe(entityType);\n    }\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\__tests__\\invariants\\sql-02-reject-unknown-columns.test.ts","messages":[{"ruleId":null,"nodeType":null,"fatal":true,"severity":2,"message":"Parsing error: \"parserOptions.project\" has been provided for @typescript-eslint/parser.\nThe file was not found in any of the provided project(s): src\\__tests__\\invariants\\sql-02-reject-unknown-columns.test.ts"}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { describe, it, expect } from 'vitest';\nimport { PostgresQueryBuilder } from '../../adapters/query-builder.js';\n\n/**\n * SQL-02: QueryBuilder rejects unknown legacy columns.\n *\n * - buildSelectQuery validates filter fields against introspected legacy schema\n * - buildBatchQuery requires schema to be set first\n * - quoteIdentifier is dialect-private (no raw interpolation)\n */\ndescribe('SQL-02: QueryBuilder rejects unknown legacy columns', () => {\n  it('should reject filters with unknown fields', () => {\n    const builder = new PostgresQueryBuilder();\n\n    builder.setLegacySchema('contacts', {\n      tableName: 'customers',\n      columns: [\n        { name: 'id', type: 'varchar' },\n        { name: 'email', type: 'varchar' },\n        { name: 'phone', type: 'varchar' },\n      ],\n    });\n\n    // Valid filter should work\n    const validQuery = builder.buildSelectQuery('contacts', [\n      { field: 'email', operator: '=', value: 'test@example.com' },\n    ]);\n    expect(validQuery.text).toContain('\"email\"');\n    expect(validQuery.values).toEqual(['test@example.com']);\n\n    // Invalid filter should throw\n    expect(() => {\n      builder.buildSelectQuery('contacts', [\n        { field: 'unknown_field', operator: '=', value: 'test' },\n      ]);\n    }).toThrow(\"Field 'unknown_field' not in legacy schema\");\n  });\n\n  it('should require schema to be set before queries', () => {\n    const builder = new PostgresQueryBuilder();\n\n    expect(() => {\n      builder.buildBatchQuery('contacts', 100, null);\n    }).toThrow('Legacy schema not set for entity type: contacts');\n  });\n\n  it('should quote identifiers to prevent injection', () => {\n    const builder = new PostgresQueryBuilder();\n\n    builder.setLegacySchema('contacts', {\n      tableName: 'cust\"omers',\n      columns: [{ name: 'na\"me', type: 'varchar' }],\n    });\n\n    const query = builder.buildSelectQuery('contacts', [\n      { field: 'na\"me', operator: '=', value: 'test' },\n    ]);\n\n    // Double-quotes should be escaped\n    expect(query.text).toContain('\"cust\"\"omers\"');\n    expect(query.text).toContain('\"na\"\"me\"');\n  });\n\n  it('should produce parameterized queries for IN operator', () => {\n    const builder = new PostgresQueryBuilder();\n\n    builder.setLegacySchema('contacts', {\n      tableName: 'customers',\n      columns: [{ name: 'email', type: 'varchar' }],\n    });\n\n    const query = builder.buildSelectQuery('contacts', [\n      { field: 'email', operator: 'IN', value: ['a@b.com', 'c@d.com'] },\n    ]);\n\n    expect(query.text).toContain('$1');\n    expect(query.text).toContain('$2');\n    expect(query.values).toEqual(['a@b.com', 'c@d.com']);\n  });\n});\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\csv-adapter.ts","messages":[{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'extractBatch' has no 'await' expression.","line":37,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":37,"endColumn":21,"suggestions":[{"messageId":"removeAsync","fix":{"range":[1173,1288],"text":"extractBatch(\n    entityType: EntityType,\n    batchSize: number,\n    cursor: Cursor\n  ): BatchResult"},"desc":"Remove 'async'."}]},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'row[this.idColumn] ?? ''' will use Object's default stringification format ('[object Object]') when stringified.","line":52,"column":24,"nodeType":"LogicalExpression","messageId":"baseToString","endLine":52,"endColumn":48},{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'getSchema' has no 'await' expression.","line":64,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":64,"endColumn":18,"suggestions":[{"messageId":"removeAsync","fix":{"range":[1900,1963],"text":"getSchema(_entityType: EntityType): LegacySchema"},"desc":"Remove 'async'."}]},{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'healthCheck' has no 'await' expression.","line":79,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":79,"endColumn":20,"suggestions":[{"messageId":"removeAsync","fix":{"range":[2256,2293],"text":"healthCheck(): boolean"},"desc":"Remove 'async'."}]}],"suppressedMessages":[],"errorCount":4,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import type { LegacyAdapter } from './legacy-adapter.js';\nimport type { Cursor } from '../types/cursor.js';\nimport type { LegacyRecord, BatchResult, EntityType } from '../types/migration-job.js';\nimport type { LegacySchema } from '../types/query.js';\n\n/**\n * CSV Legacy Adapter ÔÇö reads flat file imports.\n *\n * Accepts pre-parsed rows (Record<string, unknown>[]) rather than\n * doing file I/O directly. The caller is responsible for parsing\n * the CSV file (e.g., via papaparse) and passing the rows in.\n *\n * This keeps the migration package free of CSV parsing deps.\n */\nexport interface CsvAdapterConfig {\n  systemName: string;\n  entityType: EntityType;\n  rows: Record<string, unknown>[];\n  idColumn: string;\n}\n\nexport class CsvLegacyAdapter implements LegacyAdapter {\n  readonly systemName: string;\n  readonly transport = 'csv' as const;\n\n  private readonly entityType: EntityType;\n  private readonly rows: Record<string, unknown>[];\n  private readonly idColumn: string;\n\n  constructor(config: CsvAdapterConfig) {\n    this.systemName = config.systemName;\n    this.entityType = config.entityType;\n    this.rows = config.rows;\n    this.idColumn = config.idColumn;\n  }\n\n  async extractBatch(\n    entityType: EntityType,\n    batchSize: number,\n    cursor: Cursor\n  ): Promise<BatchResult> {\n    if (entityType !== this.entityType) {\n      throw new Error(\n        `CsvLegacyAdapter configured for '${this.entityType}', got '${entityType}'`\n      );\n    }\n\n    const offset = cursor?.type === 'offset' ? cursor.offset : 0;\n    const slice = this.rows.slice(offset, offset + batchSize);\n\n    const records: LegacyRecord[] = slice.map((row) => ({\n      legacyId: String(row[this.idColumn] ?? ''),\n      data: row,\n    }));\n\n    const nextCursor: Cursor =\n      slice.length < batchSize\n        ? null\n        : { type: 'offset', offset: offset + batchSize };\n\n    return { records, nextCursor };\n  }\n\n  async getSchema(_entityType: EntityType): Promise<LegacySchema> {\n    if (this.rows.length === 0) {\n      return { tableName: 'csv_import', columns: [] };\n    }\n\n    const firstRow = this.rows[0];\n    return {\n      tableName: 'csv_import',\n      columns: Object.keys(firstRow).map((name) => ({\n        name,\n        type: 'text',\n      })),\n    };\n  }\n\n  async healthCheck(): Promise<boolean> {\n    return this.rows.length > 0;\n  }\n\n  async close(): Promise<void> {\n    // No-op for CSV adapter\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\entity-write-adapter.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\legacy-adapter.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'tableName' is assigned a value but never used.","line":74,"column":11,"nodeType":"Identifier","messageId":"unusedVar","endLine":74,"endColumn":20},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'row['id'] ?? row['ID'] ?? row['Id'] ?? ''' will use Object's default stringification format ('[object Object]') when stringified.","line":84,"column":24,"nodeType":"LogicalExpression","messageId":"baseToString","endLine":84,"endColumn":65},{"ruleId":"@typescript-eslint/prefer-nullish-coalescing","severity":2,"message":"Prefer using nullish coalescing operator (`??=`) instead of an assignment expression, as it is simpler to read.","line":141,"column":5,"nodeType":"IfStatement","messageId":"preferNullishOverAssignment","endLine":143,"endColumn":6,"suggestions":[{"messageId":"suggestNullish","data":{"equals":"="},"fix":{"range":[4208,4286],"text":"this.pool ??= await createLegacyPool(this.config);"},"desc":"Fix to nullish coalescing operator (`??=`)."}]}],"suppressedMessages":[],"errorCount":3,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import type { QueryBuilder } from './query-builder.js';\nimport type { Cursor } from '../types/cursor.js';\nimport type { LegacyRecord, BatchResult, EntityType } from '../types/migration-job.js';\nimport type { LegacySchema } from '../types/query.js';\n\n/**\n * Legacy system adapter ÔÇö read-only interface for source systems.\n *\n * Concrete implementations connect to SQL databases, CSV files, or APIs.\n * All adapters are read-only by design (no writes to legacy systems).\n */\nexport interface LegacyAdapter {\n  readonly systemName: string;\n  readonly transport: 'sql' | 'csv' | 'api';\n\n  extractBatch(entityType: EntityType, batchSize: number, cursor: Cursor): Promise<BatchResult>;\n  getSchema(entityType: EntityType): Promise<LegacySchema>;\n  healthCheck(): Promise<boolean>;\n  close(): Promise<void>;\n}\n\n/**\n * Configuration for SQL legacy adapters.\n */\nexport interface SqlLegacyConfig {\n  systemName: string;\n  host: string;\n  port?: number;\n  database: string;\n  readOnlyUser: string;\n  password: string;\n  ssl?: boolean;\n  maxPoolSize?: number;\n  idleTimeoutMs?: number;\n  queryTimeoutMs?: number;\n}\n\n/**\n * Table allowlist ÔÇö static mapping of entity types to legacy table names.\n * Prevents SQL injection by ensuring only known tables are queried.\n */\nexport type TableAllowlist = Record<EntityType, string>;\n\n/**\n * SQL Legacy Adapter ÔÇö connects to a legacy SQL database (Postgres, MySQL, etc.)\n * using a read-only connection pool with query timeouts and table allowlisting.\n */\nexport class SqlLegacyAdapter implements LegacyAdapter {\n  readonly systemName: string;\n  readonly transport = 'sql' as const;\n\n  private readonly config: SqlLegacyConfig;\n  private readonly queryBuilder: QueryBuilder;\n  private readonly tableAllowlist: TableAllowlist;\n  private pool: LegacyPool | null = null;\n\n  constructor(\n    config: SqlLegacyConfig,\n    queryBuilder: QueryBuilder,\n    tableAllowlist: TableAllowlist\n  ) {\n    this.systemName = config.systemName;\n    this.config = config;\n    this.queryBuilder = queryBuilder;\n    this.tableAllowlist = tableAllowlist;\n  }\n\n  async extractBatch(\n    entityType: EntityType,\n    batchSize: number,\n    cursor: Cursor\n  ): Promise<BatchResult> {\n    const pool = await this.getPool();\n    const tableName = this.resolveTable(entityType);\n\n    // Set legacy schema so QueryBuilder can validate fields\n    const schema = await this.getSchema(entityType);\n    this.queryBuilder.setLegacySchema(entityType, schema);\n\n    const query = this.queryBuilder.buildBatchQuery(entityType, batchSize, cursor);\n\n    const result = await pool.query(query.text, query.values);\n    const records: LegacyRecord[] = result.rows.map((row: Record<string, unknown>) => ({\n      legacyId: String(row['id'] ?? row['ID'] ?? row['Id'] ?? ''),\n      data: row,\n    }));\n\n    const nextCursor = this.queryBuilder.extractCursor(result.rows, batchSize, cursor);\n\n    return { records, nextCursor };\n  }\n\n  async getSchema(entityType: EntityType): Promise<LegacySchema> {\n    const pool = await this.getPool();\n    const tableName = this.resolveTable(entityType);\n\n    // Introspect column names and types from information_schema\n    const result = await pool.query(\n      `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = $1 ORDER BY ordinal_position`,\n      [tableName]\n    );\n\n    return {\n      tableName,\n      columns: result.rows.map((row: Record<string, unknown>) => ({\n        name: String(row['column_name']),\n        type: String(row['data_type']),\n      })),\n    };\n  }\n\n  async healthCheck(): Promise<boolean> {\n    try {\n      const pool = await this.getPool();\n      await pool.query('SELECT 1');\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  async close(): Promise<void> {\n    if (this.pool) {\n      await this.pool.end();\n      this.pool = null;\n    }\n  }\n\n  private resolveTable(entityType: EntityType): string {\n    const table = this.tableAllowlist[entityType];\n    if (!table) {\n      throw new Error(\n        `Entity type '${entityType}' not in table allowlist. ` +\n        `Allowed: ${Object.keys(this.tableAllowlist).join(', ')}`\n      );\n    }\n    return table;\n  }\n\n  private async getPool(): Promise<LegacyPool> {\n    if (!this.pool) {\n      this.pool = await createLegacyPool(this.config);\n    }\n    return this.pool;\n  }\n}\n\n/**\n * Minimal pool interface ÔÇö abstracts pg.Pool / mysql2.Pool.\n * Consumers provide a concrete implementation via createLegacyPool.\n */\nexport interface LegacyPool {\n  query(text: string, values?: unknown[]): Promise<{ rows: Record<string, unknown>[] }>;\n  end(): Promise<void>;\n}\n\n/**\n * Pool factory ÔÇö must be set before using SqlLegacyAdapter.\n * This allows the migration package to stay free of direct pg/mysql2 deps.\n */\nlet poolFactory: ((config: SqlLegacyConfig) => Promise<LegacyPool>) | null = null;\n\nexport function setLegacyPoolFactory(\n  factory: (config: SqlLegacyConfig) => Promise<LegacyPool>\n): void {\n  poolFactory = factory;\n}\n\nasync function createLegacyPool(config: SqlLegacyConfig): Promise<LegacyPool> {\n  if (!poolFactory) {\n    throw new Error(\n      'Legacy pool factory not set. Call setLegacyPoolFactory() before using SqlLegacyAdapter.'\n    );\n  }\n  return poolFactory(config);\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\query-builder.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\adapters\\streaming-csv-adapter.ts","messages":[{"ruleId":"security/detect-non-literal-fs-filename","severity":1,"message":"Found createReadStream from package \"node:fs\" with non literal argument at index 0","line":60,"column":22,"nodeType":"CallExpression","endLine":60,"endColumn":82},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'row[this.idColumn] ?? ''' will use Object's default stringification format ('[object Object]') when stringified.","line":79,"column":30,"nodeType":"LogicalExpression","messageId":"baseToString","endLine":79,"endColumn":54}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { createReadStream } from 'node:fs';\n\nimport Papa from 'papaparse';\n\nimport type { LegacyAdapter } from './legacy-adapter.js';\nimport type { Cursor } from '../types/cursor.js';\nimport type { LegacyRecord, BatchResult, EntityType } from '../types/migration-job.js';\nimport type { LegacySchema } from '../types/query.js';\n\n/**\n * SPD-06: Streaming CSV adapter ÔÇö reads large CSV files without loading\n * the entire file into memory. Uses papaparse streaming under the hood.\n */\nexport interface StreamingCsvConfig {\n  systemName: string;\n  entityType: EntityType;\n  filePath: string;\n  idColumn: string;\n  delimiter?: string;\n  encoding?: BufferEncoding;\n}\n\nexport class StreamingCsvAdapter implements LegacyAdapter {\n  readonly systemName: string;\n  readonly transport = 'csv' as const;\n\n  private readonly entityType: EntityType;\n  private readonly filePath: string;\n  private readonly idColumn: string;\n  private readonly delimiter: string;\n  private readonly encoding: BufferEncoding;\n\n  private cachedSchema: LegacySchema | null = null;\n\n  constructor(config: StreamingCsvConfig) {\n    this.systemName = config.systemName;\n    this.entityType = config.entityType;\n    this.filePath = config.filePath;\n    this.idColumn = config.idColumn;\n    this.delimiter = config.delimiter ?? ',';\n    this.encoding = config.encoding ?? 'utf-8';\n  }\n\n  async extractBatch(\n    entityType: EntityType,\n    batchSize: number,\n    cursor: Cursor,\n  ): Promise<BatchResult> {\n    if (entityType !== this.entityType) {\n      throw new Error(\n        `StreamingCsvAdapter configured for '${this.entityType}', got '${entityType}'`,\n      );\n    }\n\n    const offset = cursor?.type === 'offset' ? cursor.offset : 0;\n    const records: LegacyRecord[] = [];\n    let rowIndex = 0;\n\n    return new Promise<BatchResult>((resolve, reject) => {\n      const stream = createReadStream(this.filePath, { encoding: this.encoding });\n\n      Papa.parse(stream, {\n        header: true,\n        delimiter: this.delimiter,\n        skipEmptyLines: true,\n        step: (result: Papa.ParseStepResult<Record<string, unknown>>, parser) => {\n          if (rowIndex < offset) {\n            rowIndex++;\n            return;\n          }\n\n          if (records.length >= batchSize) {\n            parser.abort();\n            return;\n          }\n\n          const row = result.data;\n          records.push({\n            legacyId: String(row[this.idColumn] ?? ''),\n            data: row,\n          });\n          rowIndex++;\n        },\n        complete: () => {\n          const nextCursor: Cursor =\n            records.length < batchSize\n              ? null\n              : { type: 'offset', offset: offset + batchSize };\n\n          resolve({ records, nextCursor });\n        },\n        error: (err: Error) => {\n          reject(err);\n        },\n      });\n    });\n  }\n\n  async getSchema(_entityType: EntityType): Promise<LegacySchema> {\n    if (this.cachedSchema) return this.cachedSchema;\n\n    // Read just the first row to infer schema\n    const batch = await this.extractBatch(this.entityType, 1, null);\n    const firstRow = batch.records[0];\n\n    if (!firstRow) {\n      this.cachedSchema = { tableName: 'csv_import', columns: [] };\n      return this.cachedSchema;\n    }\n\n    this.cachedSchema = {\n      tableName: 'csv_import',\n      columns: Object.keys(firstRow.data).map((name) => ({\n        name,\n        type: 'text',\n      })),\n    };\n    return this.cachedSchema;\n  }\n\n  async healthCheck(): Promise<boolean> {\n    try {\n      const batch = await this.extractBatch(this.entityType, 1, null);\n      return batch.records.length > 0;\n    } catch {\n      return false;\n    }\n  }\n\n  async close(): Promise<void> {\n    // No-op ÔÇö streams are opened/closed per extractBatch call\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\audit\\canonical-json.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\audit\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\audit\\signed-report.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\gates\\gate-chain.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\gates\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\gates\\readiness-gate.ts","messages":[{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'check' has no 'await' expression.","line":102,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":102,"endColumn":14,"suggestions":[{"messageId":"removeAsync","fix":{"range":[2974,3035],"text":"check(job: MigrationJob): ReadinessCheckResult"},"desc":"Remove 'async'."}]}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import type { PreflightGate } from './gate-chain.js';\nimport type { MigrationJob, GateResult } from '../types/migration-job.js';\n\n/**\n * OPS-03: Preflight Migration Readiness Gate.\n *\n * Runs configurable checks before migration starts:\n * - Field mapping completeness (% of legacy fields mapped)\n * - Match key null rate (flags if >threshold% of records have null match keys)\n * - Custom checks via injected functions\n *\n * Output is a structured readiness report that can be persisted.\n */\n\nexport interface ReadinessCheck {\n  readonly name: string;\n  check(job: MigrationJob): Promise<ReadinessCheckResult>;\n}\n\nexport interface ReadinessCheckResult {\n  passed: boolean;\n  severity: 'error' | 'warning' | 'info';\n  message: string;\n  details?: Record<string, unknown>;\n}\n\nexport interface ReadinessReport {\n  timestamp: string;\n  jobId: string;\n  entityType: string;\n  checks: Array<ReadinessCheckResult & { name: string }>;\n  overallPassed: boolean;\n  errorCount: number;\n  warningCount: number;\n}\n\nexport class ReadinessGate implements PreflightGate {\n  readonly name = 'OPS-03:readiness';\n  private checks: ReadinessCheck[] = [];\n  private lastReport: ReadinessReport | null = null;\n  private failOnWarnings: boolean;\n\n  constructor(opts?: { failOnWarnings?: boolean }) {\n    this.failOnWarnings = opts?.failOnWarnings ?? false;\n  }\n\n  addCheck(check: ReadinessCheck): this {\n    this.checks.push(check);\n    return this;\n  }\n\n  getLastReport(): ReadinessReport | null {\n    return this.lastReport;\n  }\n\n  async check(job: MigrationJob): Promise<GateResult> {\n    const results: Array<ReadinessCheckResult & { name: string }> = [];\n\n    for (const c of this.checks) {\n      const result = await c.check(job);\n      results.push({ ...result, name: c.name });\n    }\n\n    const errorCount = results.filter((r) => !r.passed && r.severity === 'error').length;\n    const warningCount = results.filter((r) => !r.passed && r.severity === 'warning').length;\n    const overallPassed = errorCount === 0 && (!this.failOnWarnings || warningCount === 0);\n\n    this.lastReport = {\n      timestamp: new Date().toISOString(),\n      jobId: job.id,\n      entityType: job.entityType,\n      checks: results,\n      overallPassed,\n      errorCount,\n      warningCount,\n    };\n\n    if (!overallPassed) {\n      const failedNames = results\n        .filter((r) => !r.passed && (r.severity === 'error' || (this.failOnWarnings && r.severity === 'warning')))\n        .map((r) => r.name)\n        .join(', ');\n      return { passed: false, reason: `Readiness checks failed: ${failedNames}` };\n    }\n\n    return { passed: true };\n  }\n}\n\n/**\n * Built-in readiness check: field mapping completeness.\n * Flags if fewer than threshold% of legacy fields are mapped.\n */\nexport class MappingCompletenessCheck implements ReadinessCheck {\n  readonly name = 'mapping-completeness';\n  private threshold: number;\n\n  constructor(opts?: { threshold?: number }) {\n    this.threshold = opts?.threshold ?? 0.8;\n  }\n\n  async check(job: MigrationJob): Promise<ReadinessCheckResult> {\n    const mappings = job.fieldMappings;\n    const mappedCount = Array.isArray(mappings) ? mappings.length : 0;\n\n    if (mappedCount === 0) {\n      return {\n        passed: false,\n        severity: 'error',\n        message: 'No field mappings defined',\n        details: { mappedCount: 0 },\n      };\n    }\n\n    return {\n      passed: true,\n      severity: 'info',\n      message: `${mappedCount} fields mapped`,\n      details: { mappedCount },\n    };\n  }\n}\n\n/**\n * Built-in readiness check: match key null rate.\n * Accepts a sample function that returns null rate (0ÔÇô1) for match keys.\n */\nexport class MatchKeyNullRateCheck implements ReadinessCheck {\n  readonly name = 'match-key-null-rate';\n  private threshold: number;\n  private sampleFn: (job: MigrationJob) => Promise<Record<string, number>>;\n\n  constructor(opts: {\n    threshold?: number;\n    sampleFn: (job: MigrationJob) => Promise<Record<string, number>>;\n  }) {\n    this.threshold = opts.threshold ?? 0.4;\n    this.sampleFn = opts.sampleFn;\n  }\n\n  async check(job: MigrationJob): Promise<ReadinessCheckResult> {\n    const nullRates = await this.sampleFn(job);\n    const flagged = Object.entries(nullRates).filter(([, rate]) => rate > this.threshold);\n\n    if (flagged.length > 0) {\n      const details = Object.fromEntries(flagged);\n      return {\n        passed: false,\n        severity: 'warning',\n        message: `High null rate on match keys: ${flagged.map(([k, v]) => `${k}=${(v * 100).toFixed(0)}%`).join(', ')}`,\n        details,\n      };\n    }\n\n    return {\n      passed: true,\n      severity: 'info',\n      message: 'Match key null rates within threshold',\n      details: nullRates,\n    };\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\chunked-in.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\crud-bridge.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\drizzle-pipeline-db.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\kpi-tracker.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\perf-tracker.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\pipeline-base.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\rollback-engine.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'EntityType' is defined but never used.","line":9,"column":15,"nodeType":"Identifier","messageId":"unusedVar","endLine":9,"endColumn":25,"suggestions":[{"messageId":"removeUnusedImportDeclaration","data":{"varName":"EntityType"},"fix":{"range":[188,249],"text":""},"desc":"Remove unused import declaration."}]}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import {\n  migrationJobs,\n  migrationLineage,\n  migrationRowSnapshots,\n} from 'afena-database';\nimport { and, eq } from 'drizzle-orm';\n\nimport type { CrudBridge } from './crud-bridge.js';\nimport type { EntityType } from '../types/migration-job.js';\nimport type { DbInstance } from 'afena-database';\n\n\n/**\n * Rollback result for a single migration job.\n */\nexport interface RollbackResult {\n  jobId: string;\n  entityType: string;\n  deletedCount: number;\n  restoredCount: number;\n  failedCount: number;\n  errors: Array<{ entityId: string; error: string }>;\n}\n\n/**\n * @deprecated Use CrudBridge instead. Kept for backward compatibility.\n */\nexport type MutateFn = CrudBridge['mutate'];\n\n/**\n * Rollback engine ÔÇö restores entities from write-shape snapshots.\n *\n * Two-phase rollback:\n * 1. Delete newly created records (lineage with state='committed')\n * 2. Restore updated records from before_write_core + before_write_custom snapshots\n *\n * Uses CrudBridge.mutate() for all writes to maintain audit trail integrity.\n */\nexport class RollbackEngine {\n  private readonly bridge: CrudBridge;\n\n  constructor(\n    private readonly db: DbInstance,\n    bridge: CrudBridge,\n  ) {\n    this.bridge = bridge;\n  }\n\n  async rollback(jobId: string): Promise<RollbackResult> {\n    // 1. Load job metadata\n    const jobs = await this.db\n      .select()\n      .from(migrationJobs)\n      .where(eq(migrationJobs.id, jobId));\n\n    if (jobs.length === 0) {\n      throw new Error(`Migration job '${jobId}' not found`);\n    }\n\n    const job = jobs[0];\n    if (job.status === 'rolled_back') {\n      throw new Error(`Migration job '${jobId}' already rolled back`);\n    }\n\n    const entityType = job.entityType;\n    const result: RollbackResult = {\n      jobId,\n      entityType,\n      deletedCount: 0,\n      restoredCount: 0,\n      failedCount: 0,\n      errors: [],\n    };\n\n    // 2. Delete newly created records (reverse lineage)\n    const lineageRows = await this.db\n      .select()\n      .from(migrationLineage)\n      .where(\n        and(\n          eq(migrationLineage.migrationJobId, jobId),\n          eq(migrationLineage.state, 'committed')\n        )\n      );\n\n    for (const row of lineageRows) {\n      if (!row.afenaId) continue;\n\n      try {\n        // Read current version for optimistic lock\n        const currentRow = await this.bridge.readRawRow(entityType, row.afenaId);\n        const currentVersion = currentRow\n          ? (currentRow['version'] as number | undefined)\n          : undefined;\n\n        await this.bridge.mutate({\n          actionType: `${entityType}.delete`,\n          entityType,\n          entityId: row.afenaId,\n          input: {},\n          expectedVersion: currentVersion,\n        });\n        result.deletedCount++;\n      } catch (error) {\n        result.failedCount++;\n        result.errors.push({\n          entityId: row.afenaId,\n          error: `Delete failed: ${String(error)}`,\n        });\n      }\n    }\n\n    // 3. Restore updated records from snapshots\n    const snapshots = await this.db\n      .select()\n      .from(migrationRowSnapshots)\n      .where(eq(migrationRowSnapshots.migrationJobId, jobId));\n\n    for (const snapshot of snapshots) {\n      try {\n        // Reconstruct persistable data from write-shape snapshot\n        const coreData = snapshot.beforeWriteCore as Record<string, unknown>;\n        const customData = snapshot.beforeWriteCustom as Record<string, unknown>;\n\n        const restoreData: Record<string, unknown> = {\n          ...coreData,\n          ...(Object.keys(customData).length > 0 ? { customData } : {}),\n        };\n\n        await this.bridge.mutate({\n          actionType: `${entityType}.update`,\n          entityType,\n          entityId: snapshot.entityId,\n          input: restoreData,\n          expectedVersion: snapshot.beforeVersion ?? undefined,\n        });\n\n        result.restoredCount++;\n      } catch (error) {\n        result.failedCount++;\n        result.errors.push({\n          entityId: snapshot.entityId,\n          error: `Restore failed: ${String(error)}`,\n        });\n      }\n    }\n\n    // 4. Mark job as rolled back\n    await this.db\n      .update(migrationJobs)\n      .set({ status: 'rolled_back', completedAt: new Date() })\n      .where(eq(migrationJobs.id, jobId));\n\n    // 5. Clean up lineage for this job\n    await this.db\n      .delete(migrationLineage)\n      .where(eq(migrationLineage.migrationJobId, jobId));\n\n    return result;\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\sql-migration-pipeline.ts","messages":[{"ruleId":"import/order","severity":2,"message":"There should be no empty line within import group","line":27,"column":1,"nodeType":"ImportDeclaration","endLine":27,"endColumn":77,"fix":{"range":[1126,1128],"text":""}},{"ruleId":"import/order","severity":2,"message":"`../gates/gate-chain.js` type import should occur before type import of `../strategies/conflict-detector.js`","line":27,"column":1,"nodeType":"ImportDeclaration","endLine":27,"endColumn":77,"fix":{"range":[864,1126],"text":"import type { PreflightGate, PostflightGate } from '../gates/gate-chain.js';\nimport type { ConflictDetector, Conflict, DetectorQueryFn } from '../strategies/conflict-detector.js';\nimport type { TransformChain, DataType } from '../transforms/transform-chain.js';\n"}},{"ruleId":"import/order","severity":2,"message":"`../adapters/legacy-adapter.js` type import should occur before type import of `../adapters/query-builder.js`","line":30,"column":1,"nodeType":"ImportDeclaration","endLine":30,"endColumn":68,"fix":{"range":[798,1196],"text":"import type { LegacyAdapter } from '../adapters/legacy-adapter.js';\nimport type { QueryBuilder } from '../adapters/query-builder.js';\nimport type { ConflictDetector, Conflict, DetectorQueryFn } from '../strategies/conflict-detector.js';\nimport type { TransformChain, DataType } from '../transforms/transform-chain.js';\nimport type { PreflightGate, PostflightGate } from '../gates/gate-chain.js';\n\n\n"}},{"ruleId":"import/order","severity":2,"message":"`./with-terminal-outcome.js` import should occur before type import of `./crud-bridge.js`","line":33,"column":1,"nodeType":"ImportDeclaration","endLine":33,"endColumn":66,"fix":{"range":[613,1264],"text":"import { withTerminalOutcome } from './with-terminal-outcome.js';\nimport type { CrudBridge } from './crud-bridge.js';\nimport type { PipelineDb } from './pipeline-base.js';\nimport type { EntityWriteAdapter } from '../adapters/entity-write-adapter.js';\nimport type { QueryBuilder } from '../adapters/query-builder.js';\nimport type { ConflictDetector, Conflict, DetectorQueryFn } from '../strategies/conflict-detector.js';\nimport type { TransformChain, DataType } from '../transforms/transform-chain.js';\nimport type { PreflightGate, PostflightGate } from '../gates/gate-chain.js';\n\n\nimport type { LegacyAdapter } from '../adapters/legacy-adapter.js';\n\n\n"}},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":235,"column":23,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":235,"endColumn":64},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":235,"column":23,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":235,"endColumn":55,"suggestions":[{"messageId":"suggestOptionalChain","fix":{"range":[7786,7787],"text":"?"},"desc":"Consider using the optional chain operator `?.` instead. This operator includes runtime checks, so it is safer than the compile-only non-null assertion operator."}]},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":570,"column":36,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":570,"endColumn":58},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":574,"column":43,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":574,"endColumn":65},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":650,"column":36,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":650,"endColumn":58},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":659,"column":43,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":659,"endColumn":65}],"suppressedMessages":[],"errorCount":4,"fatalErrorCount":0,"warningCount":6,"fixableErrorCount":4,"fixableWarningCount":0,"source":"\nimport {\n  migrationLineage,\n  migrationConflicts,\n  migrationConflictResolutions,\n  migrationRowSnapshots,\n  migrationCheckpoints,\n  migrationQuarantine,\n  migrationMergeExplanations,\n} from 'afena-database';\nimport { and, eq, inArray, sql } from 'drizzle-orm';\nimport pLimit from 'p-limit';\n\nimport { hashCanonical } from '../audit/canonical-json.js';\nimport { PreflightGateChain, PostflightGateChain } from '../gates/gate-chain.js';\nimport { DEFAULT_CONFLICT_THRESHOLDS } from '../types/index.js';\n\nimport { PerfTracker } from './perf-tracker.js';\nimport { MigrationPipelineBase } from './pipeline-base.js';\n\nimport type { CrudBridge } from './crud-bridge.js';\nimport type { PipelineDb } from './pipeline-base.js';\nimport type { EntityWriteAdapter } from '../adapters/entity-write-adapter.js';\nimport type { QueryBuilder } from '../adapters/query-builder.js';\nimport type { ConflictDetector, Conflict, DetectorQueryFn } from '../strategies/conflict-detector.js';\nimport type { TransformChain, DataType } from '../transforms/transform-chain.js';\nimport type { PreflightGate, PostflightGate } from '../gates/gate-chain.js';\n\n\nimport type { LegacyAdapter } from '../adapters/legacy-adapter.js';\n\n\nimport { withTerminalOutcome } from './with-terminal-outcome.js';\n\n\nimport type {\n  Cursor,\n  LegacyKey,\n  UpsertPlan,\n  UpsertAction,\n  LoadResult,\n  EntityType,\n  MigrationJob,\n  MigrationContext,\n  MigrationResult,\n  BatchResult,\n  LegacyRecord,\n  TransformedRecord,\n  GateResult,\n  RecordOutcome,\n  StepCheckpoint,\n  ConflictThresholds,\n} from '../types/index.js';\nimport type { DbInstance } from 'afena-database';\n\n/**\n * Concrete SQL migration pipeline.\n *\n * - Hardening 1: Bulk lineage prefetch + bulk conflict detection (no N+1)\n * - Hardening 2: Reservation-first create pattern (concurrency-safe)\n * - Hardening 4: Snapshot round-tripping via write-shape adapter\n */\nexport interface SqlPipelineConfig {\n  db: DbInstance;\n  pipelineDb: PipelineDb;\n  queryBuilder: QueryBuilder;\n  conflictDetector: ConflictDetector;\n  writeAdapter: EntityWriteAdapter;\n  transformChain: TransformChain;\n  crudBridge?: CrudBridge;\n  legacyAdapter?: LegacyAdapter;\n  detectorQueryFn?: DetectorQueryFn;\n  batchSize?: number;\n  fieldDataTypes?: Record<string, DataType>;\n  preflightGates?: PreflightGate[];\n  postflightGates?: PostflightGate[];\n  checkpointInterval?: number;\n  createConcurrency?: number;\n  conflictThresholds?: ConflictThresholds;\n}\n\nexport class SqlMigrationPipeline extends MigrationPipelineBase {\n  private readonly afenaDb: DbInstance;\n  private readonly queryBuilder: QueryBuilder;\n  private readonly conflictDetector: ConflictDetector;\n  private readonly writeAdapter: EntityWriteAdapter;\n  private readonly transformChain: TransformChain;\n  private readonly batchSize: number;\n  private readonly fieldDataTypes: Record<string, DataType>;\n  private readonly crudBridge: CrudBridge | null;\n  private readonly legacyAdapter: LegacyAdapter | null;\n  private readonly detectorQueryFn: DetectorQueryFn | null;\n  private readonly preflightChain: PreflightGateChain;\n  private readonly postflightChain: PostflightGateChain;\n  private readonly checkpointInterval: number;\n  private readonly createConcurrency: number;\n  private readonly conflictThresholds: ConflictThresholds;\n  readonly perf: PerfTracker = new PerfTracker();\n  private lastOutcomes: RecordOutcome[] = [];\n\n  constructor(\n    job: MigrationJob,\n    context: MigrationContext,\n    config: SqlPipelineConfig\n  ) {\n    super(job, context, config.pipelineDb);\n    this.afenaDb = config.db;\n    this.queryBuilder = config.queryBuilder;\n    this.conflictDetector = config.conflictDetector;\n    this.writeAdapter = config.writeAdapter;\n    this.transformChain = config.transformChain;\n    this.batchSize = config.batchSize ?? 500;\n    this.fieldDataTypes = config.fieldDataTypes ?? {};\n    this.crudBridge = config.crudBridge ?? null;\n    this.legacyAdapter = config.legacyAdapter ?? null;\n    this.detectorQueryFn = config.detectorQueryFn ?? null;\n    this.checkpointInterval = config.checkpointInterval ?? 50;\n    this.createConcurrency = config.createConcurrency ?? 10;\n    this.conflictThresholds = config.conflictThresholds ?? DEFAULT_CONFLICT_THRESHOLDS;\n\n    this.preflightChain = new PreflightGateChain();\n    for (const gate of config.preflightGates ?? []) {\n      this.preflightChain.addGate(gate);\n    }\n\n    this.postflightChain = new PostflightGateChain();\n    for (const gate of config.postflightGates ?? []) {\n      this.postflightChain.addGate(gate);\n    }\n  }\n\n  // ÔöÇÔöÇ Expose for signed report generation ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  getTransformChain(): TransformChain {\n    return this.transformChain;\n  }\n\n  getConflictDetector(): ConflictDetector {\n    return this.conflictDetector;\n  }\n\n  // ÔöÇÔöÇ Extract ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async extractBatch(cursor: Cursor): Promise<BatchResult> {\n    if (this.legacyAdapter) {\n      return this.legacyAdapter.extractBatch(\n        this.job.entityType,\n        this.batchSize,\n        cursor\n      );\n    }\n\n    // Fallback: use query builder (for cases where legacy adapter is not set)\n    const query = this.queryBuilder.buildBatchQuery(\n      this.job.entityType,\n      this.batchSize,\n      cursor\n    );\n    void query;\n    return { records: [], nextCursor: null };\n  }\n\n  // ÔöÇÔöÇ Transform ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async transformBatch(records: LegacyRecord[]): Promise<TransformedRecord[]> {\n    const results: TransformedRecord[] = [];\n\n    for (const record of records) {\n      const transformed: Record<string, unknown> = {};\n\n      for (const mapping of this.job.fieldMappings) {\n        const rawValue = record.data[mapping.sourceField];\n        const dataType = this.fieldDataTypes[mapping.targetField] ?? 'short_text';\n\n        const value = await this.transformChain.transform(\n          rawValue,\n          mapping.targetField,\n          dataType,\n          {\n            entityType: this.job.entityType,\n            orgId: this.context.orgId,\n          }\n        );\n\n        transformed[mapping.targetField] = value;\n      }\n\n      results.push({ legacyId: record.legacyId, data: transformed });\n    }\n\n    return results;\n  }\n\n  // ÔöÇÔöÇ Plan (Hardening 1: Bulk prefetch, no N+1) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async planUpserts(records: TransformedRecord[]): Promise<UpsertPlan> {\n    const actions: UpsertAction[] = [];\n    const legacyIds = records.map((r) => r.legacyId);\n    const systemName = this.resolveSystemName();\n\n    // 1. Bulk lineage prefetch (single query)\n    const existingLineage = await this.afenaDb\n      .select()\n      .from(migrationLineage)\n      .where(\n        and(\n          eq(migrationLineage.orgId, this.context.orgId),\n          eq(migrationLineage.entityType, this.job.entityType),\n          eq(migrationLineage.legacySystem, systemName),\n          inArray(migrationLineage.legacyId, legacyIds)\n        )\n      );\n\n    const lineageMap = new Map<string, { afenaId: string | null; state: string }>();\n    for (const line of existingLineage) {\n      lineageMap.set(line.legacyId, { afenaId: line.afenaId, state: line.state });\n    }\n\n    // 2. Separate: already-migrated vs new\n    const alreadyMigrated: TransformedRecord[] = [];\n    const newRecords: TransformedRecord[] = [];\n\n    for (const record of records) {\n      const lineage = lineageMap.get(record.legacyId);\n      if (lineage?.state === 'committed' && lineage.afenaId) {\n        alreadyMigrated.push(record);\n      } else {\n        newRecords.push(record);\n      }\n    }\n\n    // 3. Handle already-migrated\n    for (const record of alreadyMigrated) {\n      const legacyKey: LegacyKey = { legacySystem: systemName, legacyId: record.legacyId };\n      const afenaId = lineageMap.get(record.legacyId)!.afenaId!;\n\n      if (this.job.conflictStrategy === 'skip') {\n        actions.push({ kind: 'skip', legacyKey, reason: 'Already migrated' });\n      } else {\n        actions.push({ kind: 'update', targetId: afenaId, legacyKey, data: record.data });\n      }\n    }\n\n    // 4. Bulk conflict detection for new records (single query)\n    if (newRecords.length > 0) {\n      const conflicts = await this.conflictDetector.detectBulk(newRecords, {\n        orgId: this.context.orgId,\n        queryFn: this.detectorQueryFn ?? undefined,\n      });\n\n      const conflictMap = new Map<string, Conflict>();\n      for (const conflict of conflicts) {\n        conflictMap.set(conflict.legacyRecord.legacyId, conflict);\n      }\n\n      // 5. Plan actions for new records\n      for (const record of newRecords) {\n        const legacyKey: LegacyKey = { legacySystem: systemName, legacyId: record.legacyId };\n        const conflict = conflictMap.get(record.legacyId);\n\n        if (!conflict) {\n          actions.push({ kind: 'create', legacyKey, data: record.data });\n          continue;\n        }\n\n        // Conflict resolution based on strategy\n        switch (this.job.conflictStrategy) {\n          case 'skip':\n            actions.push({ kind: 'skip', legacyKey, reason: 'Conflict detected, skipped' });\n            break;\n          case 'overwrite':\n            if (conflict.matches.length > 0) {\n              const best = conflict.matches[0];\n              actions.push({\n                kind: 'update',\n                targetId: best.entityId,\n                legacyKey,\n                data: record.data,\n              });\n            }\n            break;\n          case 'merge':\n            if (conflict.matches.length > 0) {\n              const best = conflict.matches[0];\n              const score = best.score ?? 0;\n\n              if (score >= this.conflictThresholds.autoMerge) {\n                actions.push({\n                  kind: 'merge',\n                  targetId: best.entityId,\n                  legacyKey,\n                  data: record.data,\n                  evidence: {\n                    conflictId: conflict.id,\n                    chosenCandidate: best.entityId,\n                    fieldDecisions: [],\n                    resolver: 'auto',\n                  },\n                });\n                await this.writeMergeExplanation(\n                  this.job.id, this.job.entityType, record.legacyId,\n                  best.entityId, 'merged', score, best.explanations ?? [],\n                );\n              } else if (score >= this.conflictThresholds.manualReview) {\n                actions.push({\n                  kind: 'manual',\n                  legacyKey,\n                  reason: `Score ${score} below auto-merge threshold (${this.conflictThresholds.autoMerge})`,\n                  candidates: conflict.matches,\n                });\n                await this.writeMergeExplanation(\n                  this.job.id, this.job.entityType, record.legacyId,\n                  best.entityId, 'manual_review', score, best.explanations ?? [],\n                );\n              } else {\n                actions.push({ kind: 'create', legacyKey, data: record.data });\n                await this.writeMergeExplanation(\n                  this.job.id, this.job.entityType, record.legacyId,\n                  '', 'created_new', score, best.explanations ?? [],\n                );\n              }\n            }\n            break;\n          case 'manual':\n            actions.push({\n              kind: 'manual',\n              legacyKey,\n              reason: 'Manual review required',\n              candidates: conflict.matches,\n            });\n            break;\n        }\n      }\n    }\n\n    return { jobId: this.job.id, entityType: this.job.entityType, actions };\n  }\n\n  // ÔöÇÔöÇ Load (Hardening 2 + OPS-01 + OPS-02 + SPD-01) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async loadPlan(plan: UpsertPlan): Promise<LoadResult> {\n    const result: LoadResult = {\n      created: [],\n      updated: [],\n      skipped: [],\n      failed: [],\n    };\n    const outcomes: RecordOutcome[] = [];\n    const limit = pLimit(this.createConcurrency);\n\n    let i = 0;\n    while (i < plan.actions.length) {\n      // SPD-01: Collect consecutive create actions into a parallel batch\n      if (plan.actions[i].kind === 'create') {\n        const createBatch: { index: number; action: UpsertAction }[] = [];\n        while (i < plan.actions.length && plan.actions[i].kind === 'create') {\n          createBatch.push({ index: i, action: plan.actions[i] });\n          i++;\n        }\n\n        const settledResults = await Promise.allSettled(\n          createBatch.map(({ action }) =>\n            limit(() => this.executeCreate(plan, action as Extract<UpsertAction, { kind: 'create' }>, result))\n          )\n        );\n\n        for (let b = 0; b < settledResults.length; b++) {\n          const settled = settledResults[b];\n          const batchAction = createBatch[b].action;\n          const outcome: RecordOutcome = settled.status === 'fulfilled'\n            ? settled.value\n            : {\n              entityType: plan.entityType,\n              legacyId: batchAction.legacyKey.legacyId,\n              status: 'quarantined',\n              errorClass: 'permanent',\n              errorCode: settled.reason instanceof Error ? settled.reason.message.slice(0, 100) : 'UNKNOWN',\n              failureStage: 'load',\n            };\n          const batchIdx = createBatch[b].index;\n\n          if (outcome.status === 'quarantined') {\n            await this.writeQuarantineRow(plan.jobId, plan.entityType, batchAction, outcome);\n            result.failed.push({ legacyId: batchAction.legacyKey.legacyId, error: outcome.errorCode ?? 'quarantined' });\n          }\n\n          outcomes.push(outcome);\n\n          if (this.checkpointInterval > 0 && batchIdx % this.checkpointInterval === 0 && batchIdx > 0) {\n            await this.writeCheckpoint(plan.jobId, plan.entityType, {\n              cursor: this.job.checkpointCursor,\n              batchIndex: 0,\n              loadedUpTo: batchIdx,\n              transformVersion: hashCanonical(this.transformChain.getSteps().map(s => ({ name: s.name, order: s.order }))),\n            });\n          }\n        }\n        continue;\n      }\n\n      // Sequential processing for skip, manual, update, merge\n      const action = plan.actions[i];\n      const entityType = plan.entityType;\n      const legacyId = action.legacyKey.legacyId;\n\n      let outcome: RecordOutcome;\n\n      switch (action.kind) {\n        case 'skip': {\n          result.skipped.push({ legacyId, reason: action.reason });\n          outcome = { entityType, legacyId, status: 'skipped', action: 'skip' };\n          break;\n        }\n\n        case 'manual': {\n          await this.afenaDb.insert(migrationConflicts).values({\n            orgId: this.context.orgId,\n            migrationJobId: plan.jobId,\n            entityType,\n            legacyRecord: { legacyKey: action.legacyKey, data: {} },\n            candidateMatches: action.candidates ?? [],\n            confidence: 'medium',\n            resolution: 'manual_review',\n          });\n          result.skipped.push({ legacyId, reason: action.reason });\n          outcome = { entityType, legacyId, status: 'manual_review' };\n          break;\n        }\n\n        case 'update':\n        case 'merge': {\n          const actionKind = action.kind;\n          outcome = await withTerminalOutcome({ entityType, legacyId }, async () => {\n            const endMutate = this.perf.start(`mutate_${actionKind}_ms`);\n            try {\n              await this.captureSnapshot(plan.jobId, entityType, action.targetId);\n\n              if (this.crudBridge) {\n                const currentRow = await this.crudBridge.readRawRow(entityType, action.targetId);\n                const currentVersion = currentRow\n                  ? (currentRow['version'] as number | undefined)\n                  : undefined;\n\n                const mutateResult = await this.crudBridge.mutate({\n                  actionType: `${entityType}.update`,\n                  entityType,\n                  entityId: action.targetId,\n                  input: action.data,\n                  expectedVersion: currentVersion,\n                });\n\n                if (mutateResult.status !== 'ok') {\n                  throw Object.assign(\n                    new Error(`Update failed: ${mutateResult.errorCode ?? 'unknown'} ÔÇö ${mutateResult.errorMessage ?? ''}`),\n                    { stage: 'load' }\n                  );\n                }\n              }\n\n              result.updated.push({ legacyId, afenaId: action.targetId });\n\n              if (actionKind === 'merge' && 'evidence' in action && action.evidence) {\n                await this.afenaDb.insert(migrationConflictResolutions).values({\n                  orgId: this.context.orgId,\n                  migrationJobId: plan.jobId,\n                  conflictId: action.evidence.conflictId,\n                  decision: 'merged',\n                  chosenCandidateId: action.evidence.chosenCandidate,\n                  fieldDecisions: action.evidence.fieldDecisions,\n                  resolver: action.evidence.resolver,\n                });\n              }\n\n              return {\n                entityType, legacyId,\n                status: 'loaded' as const,\n                action: actionKind,\n                targetId: action.targetId,\n              };\n            } finally {\n              endMutate();\n            }\n          });\n          break;\n        }\n\n        default: {\n          outcome = { entityType, legacyId, status: 'skipped' };\n          break;\n        }\n      }\n\n      if (outcome.status === 'quarantined') {\n        await this.writeQuarantineRow(plan.jobId, plan.entityType, action, outcome);\n        result.failed.push({ legacyId, error: outcome.errorCode ?? 'quarantined' });\n      }\n\n      outcomes.push(outcome);\n\n      if (this.checkpointInterval > 0 && i % this.checkpointInterval === 0 && i > 0) {\n        await this.writeCheckpoint(plan.jobId, plan.entityType, {\n          cursor: this.job.checkpointCursor,\n          batchIndex: 0,\n          loadedUpTo: i,\n          transformVersion: hashCanonical(this.transformChain.getSteps().map(s => ({ name: s.name, order: s.order }))),\n        });\n      }\n\n      i++;\n    }\n\n    // P0-3: Batch completion checkpoint ÔÇö advance cursor, reset loadedUpTo\n    if (this.checkpointInterval > 0 && outcomes.length > 0) {\n      await this.writeCheckpoint(plan.jobId, plan.entityType, {\n        cursor: this.job.checkpointCursor,\n        batchIndex: 0,\n        loadedUpTo: plan.actions.length,\n        transformVersion: hashCanonical(this.transformChain.getSteps().map(s => ({ name: s.name, order: s.order }))),\n      });\n    }\n\n    this.lastOutcomes = outcomes;\n    return result;\n  }\n\n  // ÔöÇÔöÇ SPD-01: Parallel create execution ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  private async executeCreate(\n    plan: UpsertPlan,\n    action: Extract<UpsertAction, { kind: 'create' }>,\n    result: LoadResult,\n  ): Promise<RecordOutcome> {\n    const entityType = plan.entityType;\n    const legacyId = action.legacyKey.legacyId;\n\n    return withTerminalOutcome({ entityType, legacyId }, async () => {\n      const endMutate = this.perf.start('mutate_create_ms');\n      try {\n        const reservation = await this.reserveLineage(\n          plan.jobId, entityType, action.legacyKey\n        );\n\n        if (!reservation.isWinner) {\n          result.skipped.push({ legacyId, reason: 'Concurrent create detected, skipped' });\n          return { entityType, legacyId, status: 'skipped' as const, action: 'skip' as const };\n        }\n\n        try {\n          let afenaId: string;\n\n          if (this.crudBridge) {\n            const mutateResult = await this.crudBridge.mutate({\n              actionType: `${entityType}.create`,\n              entityType,\n              input: action.data,\n              idempotencyKey: `mig:${plan.jobId}:${legacyId}`,\n            });\n\n            if (mutateResult.status !== 'ok' || !mutateResult.entityId) {\n              throw Object.assign(\n                new Error(`Create failed: ${mutateResult.errorCode ?? 'unknown'} ÔÇö ${mutateResult.errorMessage ?? ''}`),\n                { stage: 'load' }\n              );\n            }\n            afenaId = mutateResult.entityId;\n          } else {\n            afenaId = crypto.randomUUID();\n          }\n\n          await this.commitLineage(reservation.lineageId!, afenaId);\n          result.created.push({ legacyId, afenaId });\n          return { entityType, legacyId, status: 'loaded' as const, action: 'create' as const, targetId: afenaId };\n        } catch (createError) {\n          await this.db.deleteReservation(reservation.lineageId!);\n          throw createError;\n        }\n      } finally {\n        endMutate();\n      }\n    });\n  }\n\n  getLastOutcomes(): RecordOutcome[] {\n    return this.lastOutcomes;\n  }\n\n  // ÔöÇÔöÇ P2-1: Quarantine replay ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async replayQuarantinedRecord(quarantineId: string): Promise<RecordOutcome> {\n    const rows = await this.afenaDb\n      .select()\n      .from(migrationQuarantine)\n      .where(\n        and(\n          eq(migrationQuarantine.id, quarantineId),\n          eq(migrationQuarantine.status, 'quarantined'),\n        )\n      )\n      .limit(1);\n\n    const row = rows[0];\n    if (!row) {\n      return {\n        entityType: 'unknown',\n        legacyId: 'unknown',\n        status: 'skipped',\n        action: 'skip',\n      };\n    }\n\n    const legacyKey = { legacySystem: row.legacySystem, legacyId: row.legacyId };\n    const data = row.recordData as Record<string, unknown>;\n\n    const outcome = await withTerminalOutcome(\n      { entityType: row.entityType, legacyId: row.legacyId },\n      async () => {\n        const reservation = await this.reserveLineage(\n          row.migrationJobId, row.entityType, legacyKey\n        );\n\n        if (!reservation.isWinner) {\n          return {\n            entityType: row.entityType,\n            legacyId: row.legacyId,\n            status: 'skipped' as const,\n            action: 'skip' as const,\n          };\n        }\n\n        try {\n          let afenaId: string;\n          if (this.crudBridge) {\n            const mutateResult = await this.crudBridge.mutate({\n              actionType: `${row.entityType}.create`,\n              entityType: row.entityType,\n              input: data,\n              idempotencyKey: `mig:replay:${quarantineId}`,\n            });\n            if (mutateResult.status !== 'ok' || !mutateResult.entityId) {\n              throw Object.assign(\n                new Error(`Replay create failed: ${mutateResult.errorCode ?? 'unknown'}`),\n                { stage: 'load' }\n              );\n            }\n            afenaId = mutateResult.entityId;\n          } else {\n            afenaId = crypto.randomUUID();\n          }\n\n          await this.commitLineage(reservation.lineageId!, afenaId);\n          return {\n            entityType: row.entityType,\n            legacyId: row.legacyId,\n            status: 'loaded' as const,\n            action: 'create' as const,\n            targetId: afenaId,\n          };\n        } catch (err) {\n          await this.db.deleteReservation(reservation.lineageId!);\n          throw err;\n        }\n      },\n    );\n\n    // Mark quarantine row resolved on success\n    if (outcome.status === 'loaded') {\n      await this.afenaDb\n        .update(migrationQuarantine)\n        .set({ status: 'resolved', resolvedAt: new Date() })\n        .where(eq(migrationQuarantine.id, quarantineId));\n    }\n\n    return outcome;\n  }\n\n  // ÔöÇÔöÇ Snapshot capture (Hardening 4) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async captureSnapshot(\n    jobId: string,\n    entityType: EntityType,\n    entityId: string\n  ): Promise<void> {\n    // Read raw DB row via bridge (not DTO) ÔÇö uses write adapter to separate core/custom\n    let rawRow: Record<string, unknown> = {};\n\n    if (this.crudBridge) {\n      const row = await this.crudBridge.readRawRow(entityType, entityId);\n      if (row) rawRow = row;\n    }\n\n    const { core, custom } = this.writeAdapter.toWriteShape(rawRow);\n\n    await this.afenaDb\n      .insert(migrationRowSnapshots)\n      .values({\n        orgId: this.context.orgId,\n        migrationJobId: jobId,\n        entityType,\n        entityId,\n        beforeWriteCore: core,\n        beforeWriteCustom: custom,\n        beforeVersion: typeof rawRow['version'] === 'number' ? rawRow['version'] : null,\n      })\n      .onConflictDoNothing();\n  }\n\n  // ÔöÇÔöÇ Gates ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  protected async runPreflightGates(): Promise<GateResult> {\n    return this.preflightChain.run(this.job);\n  }\n\n  protected async runPostflightGates(result: MigrationResult): Promise<GateResult> {\n    return this.postflightChain.run(this.job, result);\n  }\n\n  // ÔöÇÔöÇ Quarantine persistence ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  private async writeQuarantineRow(\n    jobId: string,\n    entityType: string,\n    action: UpsertAction,\n    outcome: RecordOutcome,\n  ): Promise<void> {\n    const errorHash = hashCanonical({\n      code: outcome.errorCode,\n      stage: outcome.failureStage,\n    });\n\n    await this.afenaDb\n      .insert(migrationQuarantine)\n      .values({\n        orgId: this.context.orgId,\n        migrationJobId: jobId,\n        entityType,\n        legacyId: action.legacyKey.legacyId,\n        legacySystem: this.resolveSystemName(),\n        recordData: 'data' in action ? action.data : {},\n        transformVersion: hashCanonical(\n          this.transformChain.getSteps().map(s => ({ name: s.name, order: s.order }))\n        ),\n        failureStage: outcome.failureStage ?? 'load',\n        errorClass: outcome.errorClass ?? 'permanent',\n        errorCode: outcome.errorCode ?? 'UNKNOWN',\n        errorMessage: outcome.errorCode,\n        lastErrorHash: errorHash,\n        status: 'quarantined',\n      })\n      .onConflictDoNothing();\n  }\n\n  // ÔöÇÔöÇ Checkpoint persistence (OPS-02) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  private async writeCheckpoint(\n    jobId: string,\n    entityType: string,\n    checkpoint: StepCheckpoint,\n  ): Promise<void> {\n    await this.afenaDb\n      .insert(migrationCheckpoints)\n      .values({\n        orgId: this.context.orgId,\n        migrationJobId: jobId,\n        entityType,\n        cursorJson: checkpoint.cursor ?? {},\n        batchIndex: checkpoint.batchIndex,\n        loadedUpTo: checkpoint.loadedUpTo,\n        transformVersion: checkpoint.transformVersion,\n        ...(checkpoint.planFingerprint ? { planFingerprint: checkpoint.planFingerprint } : {}),\n      })\n      .onConflictDoUpdate({\n        target: [migrationCheckpoints.migrationJobId, migrationCheckpoints.entityType],\n        set: {\n          cursorJson: sql`excluded.cursor_json`,\n          batchIndex: sql`excluded.batch_index`,\n          loadedUpTo: sql`excluded.loaded_up_to`,\n          transformVersion: sql`excluded.transform_version`,\n          planFingerprint: sql`excluded.plan_fingerprint`,\n          updatedAt: sql`now()`,\n        },\n      });\n  }\n\n  async loadCheckpoint(\n    jobId: string,\n    entityType: string,\n  ): Promise<StepCheckpoint | null> {\n    const rows = await this.afenaDb\n      .select()\n      .from(migrationCheckpoints)\n      .where(\n        and(\n          eq(migrationCheckpoints.migrationJobId, jobId),\n          eq(migrationCheckpoints.entityType, entityType),\n        )\n      )\n      .limit(1);\n\n    const row = rows[0];\n    if (!row) return null;\n\n    return {\n      cursor: row.cursorJson as StepCheckpoint['cursor'],\n      batchIndex: row.batchIndex,\n      loadedUpTo: row.loadedUpTo,\n      transformVersion: row.transformVersion,\n      ...(row.planFingerprint ? { planFingerprint: row.planFingerprint } : {}),\n    };\n  }\n\n  // ÔöÇÔöÇ Merge explanation persistence (ACC-05) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  private async writeMergeExplanation(\n    jobId: string,\n    entityType: string,\n    legacyId: string,\n    targetId: string,\n    decision: 'merged' | 'manual_review' | 'created_new',\n    scoreTotal: number,\n    reasons: Array<{ field: string; matchType: string; scoreContribution: number; legacyValue?: string; candidateValue?: string }>,\n  ): Promise<void> {\n    await this.afenaDb\n      .insert(migrationMergeExplanations)\n      .values({\n        orgId: this.context.orgId,\n        migrationJobId: jobId,\n        entityType,\n        legacyId,\n        targetId,\n        decision,\n        scoreTotal,\n        reasons,\n      })\n      .onConflictDoNothing();\n  }\n\n  // ÔöÇÔöÇ Private helpers ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  private resolveSystemName(): string {\n    return this.job.sourceConfig.systemName;\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\pipeline\\with-terminal-outcome.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\queries\\control-plane.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'sql' is defined but never used.","line":9,"column":32,"nodeType":"Identifier","messageId":"unusedVar","endLine":9,"endColumn":35,"suggestions":[{"messageId":"removeUnusedVar","data":{"varName":"sql"},"fix":{"range":[186,191],"text":""},"desc":"Remove unused variable \"sql\"."}]}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import {\n  getDb,\n  migrationJobs,\n  migrationQuarantine,\n  migrationCheckpoints,\n  migrationMergeExplanations,\n  migrationLineage,\n} from 'afena-database';\nimport { and, count, desc, eq, sql } from 'drizzle-orm';\n\nimport type { DbInstance } from 'afena-database';\n\n/**\n * SPD-05: Create a control plane instance routed to the read replica by default.\n * Use `forcePrimary: true` for read-after-write consistency.\n */\nexport function createControlPlane(options?: { forcePrimary?: boolean }): MigrationControlPlane {\n  return new MigrationControlPlane(getDb(options));\n}\n\n/**\n * OPS-04: Control Plane ÔÇö Read-only query layer for migration observability.\n *\n * All methods are tenant-scoped (orgId required).\n * Designed to be consumed by API routes or server actions.\n */\n\n// ÔöÇÔöÇ Types ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport interface JobSummary {\n  id: string;\n  entityType: string;\n  status: string;\n  recordsSuccess: number;\n  recordsFailed: number;\n  createdAt: Date;\n  startedAt: Date | null;\n  completedAt: Date | null;\n}\n\nexport interface QuarantineEntry {\n  id: string;\n  entityType: string;\n  legacyId: string;\n  legacySystem: string;\n  failureStage: string;\n  errorClass: string;\n  errorCode: string;\n  errorMessage: string | null;\n  retryCount: number;\n  status: string;\n  createdAt: Date;\n}\n\nexport interface CheckpointEntry {\n  entityType: string;\n  batchIndex: number;\n  loadedUpTo: number;\n  transformVersion: string;\n  updatedAt: Date;\n}\n\nexport interface MergeExplanationEntry {\n  id: string;\n  entityType: string;\n  legacyId: string;\n  targetId: string;\n  decision: string;\n  scoreTotal: number;\n  reasons: unknown;\n  createdAt: Date;\n}\n\nexport interface JobStats {\n  totalRecords: number;\n  quarantinedCount: number;\n  mergeExplanationCount: number;\n  lineageCommittedCount: number;\n  lineageReservedCount: number;\n}\n\n// ÔöÇÔöÇ Query class ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class MigrationControlPlane {\n  constructor(private readonly db: DbInstance) { }\n\n  // ÔöÇÔöÇ Jobs ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async listJobs(orgId: string, opts?: { limit?: number; offset?: number }): Promise<JobSummary[]> {\n    const limit = opts?.limit ?? 50;\n    const offset = opts?.offset ?? 0;\n\n    const rows = await this.db\n      .select({\n        id: migrationJobs.id,\n        entityType: migrationJobs.entityType,\n        status: migrationJobs.status,\n        recordsSuccess: migrationJobs.recordsSuccess,\n        recordsFailed: migrationJobs.recordsFailed,\n        createdAt: migrationJobs.createdAt,\n        startedAt: migrationJobs.startedAt,\n        completedAt: migrationJobs.completedAt,\n      })\n      .from(migrationJobs)\n      .where(eq(migrationJobs.orgId, orgId))\n      .orderBy(desc(migrationJobs.createdAt))\n      .limit(limit)\n      .offset(offset);\n\n    return rows;\n  }\n\n  async getJob(orgId: string, jobId: string): Promise<JobSummary | null> {\n    const rows = await this.db\n      .select({\n        id: migrationJobs.id,\n        entityType: migrationJobs.entityType,\n        status: migrationJobs.status,\n        recordsSuccess: migrationJobs.recordsSuccess,\n        recordsFailed: migrationJobs.recordsFailed,\n        createdAt: migrationJobs.createdAt,\n        startedAt: migrationJobs.startedAt,\n        completedAt: migrationJobs.completedAt,\n      })\n      .from(migrationJobs)\n      .where(and(eq(migrationJobs.orgId, orgId), eq(migrationJobs.id, jobId)))\n      .limit(1);\n\n    return rows[0] ?? null;\n  }\n\n  // ÔöÇÔöÇ Quarantine ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async listQuarantine(\n    orgId: string,\n    jobId: string,\n    opts?: { status?: string; limit?: number; offset?: number },\n  ): Promise<QuarantineEntry[]> {\n    const limit = opts?.limit ?? 50;\n    const offset = opts?.offset ?? 0;\n\n    const conditions = [\n      eq(migrationQuarantine.orgId, orgId),\n      eq(migrationQuarantine.migrationJobId, jobId),\n    ];\n    if (opts?.status) {\n      conditions.push(eq(migrationQuarantine.status, opts.status));\n    }\n\n    const rows = await this.db\n      .select({\n        id: migrationQuarantine.id,\n        entityType: migrationQuarantine.entityType,\n        legacyId: migrationQuarantine.legacyId,\n        legacySystem: migrationQuarantine.legacySystem,\n        failureStage: migrationQuarantine.failureStage,\n        errorClass: migrationQuarantine.errorClass,\n        errorCode: migrationQuarantine.errorCode,\n        errorMessage: migrationQuarantine.errorMessage,\n        retryCount: migrationQuarantine.retryCount,\n        status: migrationQuarantine.status,\n        createdAt: migrationQuarantine.createdAt,\n      })\n      .from(migrationQuarantine)\n      .where(and(...conditions))\n      .orderBy(desc(migrationQuarantine.createdAt))\n      .limit(limit)\n      .offset(offset);\n\n    return rows;\n  }\n\n  // ÔöÇÔöÇ Checkpoints ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async getCheckpoints(orgId: string, jobId: string): Promise<CheckpointEntry[]> {\n    const rows = await this.db\n      .select({\n        entityType: migrationCheckpoints.entityType,\n        batchIndex: migrationCheckpoints.batchIndex,\n        loadedUpTo: migrationCheckpoints.loadedUpTo,\n        transformVersion: migrationCheckpoints.transformVersion,\n        updatedAt: migrationCheckpoints.updatedAt,\n      })\n      .from(migrationCheckpoints)\n      .where(\n        and(\n          eq(migrationCheckpoints.orgId, orgId),\n          eq(migrationCheckpoints.migrationJobId, jobId),\n        ),\n      );\n\n    return rows;\n  }\n\n  // ÔöÇÔöÇ Merge Explanations ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async listMergeExplanations(\n    orgId: string,\n    jobId: string,\n    opts?: { decision?: string; limit?: number; offset?: number },\n  ): Promise<MergeExplanationEntry[]> {\n    const limit = opts?.limit ?? 50;\n    const offset = opts?.offset ?? 0;\n\n    const conditions = [\n      eq(migrationMergeExplanations.orgId, orgId),\n      eq(migrationMergeExplanations.migrationJobId, jobId),\n    ];\n    if (opts?.decision) {\n      conditions.push(eq(migrationMergeExplanations.decision, opts.decision));\n    }\n\n    const rows = await this.db\n      .select({\n        id: migrationMergeExplanations.id,\n        entityType: migrationMergeExplanations.entityType,\n        legacyId: migrationMergeExplanations.legacyId,\n        targetId: migrationMergeExplanations.targetId,\n        decision: migrationMergeExplanations.decision,\n        scoreTotal: migrationMergeExplanations.scoreTotal,\n        reasons: migrationMergeExplanations.reasons,\n        createdAt: migrationMergeExplanations.createdAt,\n      })\n      .from(migrationMergeExplanations)\n      .where(and(...conditions))\n      .orderBy(desc(migrationMergeExplanations.createdAt))\n      .limit(limit)\n      .offset(offset);\n\n    return rows;\n  }\n\n  // ÔöÇÔöÇ Aggregate Stats ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\n  async getJobStats(orgId: string, jobId: string): Promise<JobStats> {\n    const [quarantined, explanations, committed, reserved] = await Promise.all([\n      this.db\n        .select({ value: count() })\n        .from(migrationQuarantine)\n        .where(\n          and(\n            eq(migrationQuarantine.orgId, orgId),\n            eq(migrationQuarantine.migrationJobId, jobId),\n          ),\n        ),\n      this.db\n        .select({ value: count() })\n        .from(migrationMergeExplanations)\n        .where(\n          and(\n            eq(migrationMergeExplanations.orgId, orgId),\n            eq(migrationMergeExplanations.migrationJobId, jobId),\n          ),\n        ),\n      this.db\n        .select({ value: count() })\n        .from(migrationLineage)\n        .where(\n          and(\n            eq(migrationLineage.orgId, orgId),\n            eq(migrationLineage.migrationJobId, jobId),\n            eq(migrationLineage.state, 'committed'),\n          ),\n        ),\n      this.db\n        .select({ value: count() })\n        .from(migrationLineage)\n        .where(\n          and(\n            eq(migrationLineage.orgId, orgId),\n            eq(migrationLineage.migrationJobId, jobId),\n            eq(migrationLineage.state, 'reserved'),\n          ),\n        ),\n    ]);\n\n    const job = await this.getJob(orgId, jobId);\n\n    return {\n      totalRecords: (job?.recordsSuccess ?? 0) + (job?.recordsFailed ?? 0),\n      quarantinedCount: quarantined[0]?.value ?? 0,\n      mergeExplanationCount: explanations[0]?.value ?? 0,\n      lineageCommittedCount: committed[0]?.value ?? 0,\n      lineageReservedCount: reserved[0]?.value ?? 0,\n    };\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\queries\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\strategies\\conflict-detector.ts","messages":[{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'c['email']' will use Object's default stringification format ('[object Object]') when stringified.","line":88,"column":28,"nodeType":"MemberExpression","messageId":"baseToString","endLine":88,"endColumn":38},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":90,"column":9,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":90,"endColumn":26,"suggestions":[{"messageId":"suggestOptionalChain","fix":{"range":[2930,2931],"text":"?"},"desc":"Consider using the optional chain operator `?.` instead. This operator includes runtime checks, so it is safer than the compile-only non-null assertion operator."}]},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'c['phone']' will use Object's default stringification format ('[object Object]') when stringified.","line":93,"column":28,"nodeType":"MemberExpression","messageId":"baseToString","endLine":93,"endColumn":38},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":95,"column":9,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":95,"endColumn":26,"suggestions":[{"messageId":"suggestOptionalChain","fix":{"range":[3090,3091],"text":"?"},"desc":"Consider using the optional chain operator `?.` instead. This operator includes runtime checks, so it is safer than the compile-only non-null assertion operator."}]},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['email']' will use Object's default stringification format ('[object Object]') when stringified.","line":103,"column":51,"nodeType":"MemberExpression","messageId":"baseToString","endLine":103,"endColumn":71},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['phone']' will use Object's default stringification format ('[object Object]') when stringified.","line":104,"column":51,"nodeType":"MemberExpression","messageId":"baseToString","endLine":104,"endColumn":71},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":107,"column":25,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":107,"endColumn":44},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":122,"column":25,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":122,"endColumn":44},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":185,"column":7,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":185,"endColumn":29,"suggestions":[{"messageId":"suggestOptionalChain","fix":{"range":[6285,6286],"text":"?"},"desc":"Consider using the optional chain operator `?.` instead. This operator includes runtime checks, so it is safer than the compile-only non-null assertion operator."}]},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['invoiceNumber']' will use Object's default stringification format ('[object Object]') when stringified.","line":190,"column":60,"nodeType":"MemberExpression","messageId":"baseToString","endLine":190,"endColumn":88},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":193,"column":25,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":193,"endColumn":50},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['vendorId']' will use Object's default stringification format ('[object Object]') when stringified.","line":203,"column":47,"nodeType":"MemberExpression","messageId":"baseToString","endLine":203,"endColumn":70},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['vendorId']' will use Object's default stringification format ('[object Object]') when stringified.","line":209,"column":33,"nodeType":"MemberExpression","messageId":"baseToString","endLine":209,"endColumn":56},{"ruleId":"@typescript-eslint/no-base-to-string","severity":2,"message":"'record.data['sku']' will use Object's default stringification format ('[object Object]') when stringified.","line":260,"column":47,"nodeType":"MemberExpression","messageId":"baseToString","endLine":260,"endColumn":65},{"ruleId":"@typescript-eslint/no-non-null-assertion","severity":1,"message":"Forbidden non-null assertion.","line":263,"column":25,"nodeType":"TSNonNullExpression","messageId":"noNonNull","endLine":263,"endColumn":40},{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'detectBulk' has no 'await' expression.","line":295,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":295,"endColumn":19,"suggestions":[{"messageId":"removeAsync","fix":{"range":[9543,9582],"text":"detectBulk(): Conflict[]"},"desc":"Remove 'async'."}]}],"suppressedMessages":[],"errorCount":9,"fatalErrorCount":0,"warningCount":7,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import type { MatchExplanation } from '../types/match-explanation.js';\nimport type { EntityType, TransformedRecord } from '../types/migration-job.js';\nimport type { MatchCandidate } from '../types/upsert-plan.js';\n\n/**\n * Bulk query function injected into conflict detectors.\n * Executes a single query: SELECT id, ...matchFields FROM <table>\n * WHERE org_id = :orgId AND (<field> IN (:values) OR ...)\n *\n * The caller (app layer) provides this using Drizzle or raw SQL.\n * This keeps conflict-detector.ts free of direct DB imports.\n */\nexport interface DetectorQueryFn {\n  (params: {\n    entityType: string;\n    orgId: string;\n    matchFields: string[];\n    matchValues: Record<string, unknown[]>;\n  }): Promise<Array<Record<string, unknown>>>;\n}\n\n/**\n * Conflict detected during migration\n */\nexport interface Conflict {\n  id: string;\n  legacyRecord: TransformedRecord;\n  matches: MatchCandidate[];\n  confidence: 'high' | 'medium' | 'low';\n}\n\n/**\n * Context passed to conflict detectors\n */\nexport interface DetectorContext {\n  orgId: string;\n  queryFn?: DetectorQueryFn;\n}\n\n/**\n * Fix 3: Entity-agnostic conflict detection as Strategy.\n *\n * - matchKeys: declares which fields the detector relies on (for signed report fingerprint)\n * - detectBulk: single query for all candidates in a batch (no N+1)\n * - Registry must be total (every EntityType has an entry)\n */\nexport interface ConflictDetector {\n  readonly name: string;\n  readonly entityType: EntityType;\n  readonly matchKeys: readonly string[];\n  detectBulk(records: TransformedRecord[], ctx: DetectorContext): Promise<Conflict[]>;\n}\n\n// ÔöÇÔöÇ Contacts (email + phone) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class ContactsConflictDetector implements ConflictDetector {\n  readonly name = 'contacts_email_phone';\n  readonly entityType: EntityType = 'contacts';\n  readonly matchKeys = ['email', 'phone'] as const;\n\n  async detectBulk(\n    records: TransformedRecord[],\n    ctx: DetectorContext\n  ): Promise<Conflict[]> {\n    if (!ctx.queryFn || records.length === 0) return [];\n\n    // 1. Collect match values from batch\n    const emails = records.map((r) => r.data['email']).filter(Boolean);\n    const phones = records.map((r) => r.data['phone']).filter(Boolean);\n\n    if (emails.length === 0 && phones.length === 0) return [];\n\n    // 2. Single bulk query\n    const candidates = await ctx.queryFn({\n      entityType: 'contacts',\n      orgId: ctx.orgId,\n      matchFields: ['id', 'email', 'phone', 'name'],\n      matchValues: { email: emails, phone: phones },\n    });\n\n    if (candidates.length === 0) return [];\n\n    // 3. Build lookup maps\n    const byEmail = new Map<string, Array<Record<string, unknown>>>();\n    const byPhone = new Map<string, Array<Record<string, unknown>>>();\n    for (const c of candidates) {\n      if (c['email']) {\n        const key = String(c['email']).toLowerCase();\n        if (!byEmail.has(key)) byEmail.set(key, []);\n        byEmail.get(key)!.push(c);\n      }\n      if (c['phone']) {\n        const key = String(c['phone']);\n        if (!byPhone.has(key)) byPhone.set(key, []);\n        byPhone.get(key)!.push(c);\n      }\n    }\n\n    // 4. Match records to candidates with scoring + explanations\n    const conflicts: Conflict[] = [];\n    for (const record of records) {\n      const matchSet = new Map<string, { entity: Record<string, unknown>; score: number; explanations: MatchExplanation[] }>();\n      const email = record.data['email'] ? String(record.data['email']).toLowerCase() : null;\n      const phone = record.data['phone'] ? String(record.data['phone']) : null;\n\n      if (email && byEmail.has(email)) {\n        for (const c of byEmail.get(email)!) {\n          const id = String(c['id']);\n          const existing = matchSet.get(id) ?? { entity: c, score: 0, explanations: [] };\n          existing.score += 40;\n          existing.explanations.push({\n            field: 'email',\n            matchType: 'normalized',\n            scoreContribution: 40,\n            legacyValue: email,\n            candidateValue: String(c['email']),\n          });\n          matchSet.set(id, existing);\n        }\n      }\n      if (phone && byPhone.has(phone)) {\n        for (const c of byPhone.get(phone)!) {\n          const id = String(c['id']);\n          const existing = matchSet.get(id) ?? { entity: c, score: 0, explanations: [] };\n          existing.score += 20;\n          existing.explanations.push({\n            field: 'phone',\n            matchType: 'exact',\n            scoreContribution: 20,\n            legacyValue: phone,\n            candidateValue: String(c['phone']),\n          });\n          matchSet.set(id, existing);\n        }\n      }\n\n      if (matchSet.size > 0) {\n        const matches: MatchCandidate[] = Array.from(matchSet.entries())\n          .map(([entityId, m]) => ({ entityId, entity: m.entity, score: m.score, explanations: m.explanations }))\n          .sort((a, b) => b.score - a.score);\n\n        const bestScore = matches[0].score;\n        conflicts.push({\n          id: crypto.randomUUID(),\n          legacyRecord: record,\n          matches,\n          confidence: bestScore >= 40 ? 'high' : bestScore >= 20 ? 'medium' : 'low',\n        });\n      }\n    }\n\n    return conflicts;\n  }\n}\n\n// ÔöÇÔöÇ Invoices (invoice_number + vendor) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class InvoicesConflictDetector implements ConflictDetector {\n  readonly name = 'invoices_number_vendor';\n  readonly entityType: EntityType = 'invoices';\n  readonly matchKeys = ['invoiceNumber', 'vendorId'] as const;\n\n  async detectBulk(\n    records: TransformedRecord[],\n    ctx: DetectorContext\n  ): Promise<Conflict[]> {\n    if (!ctx.queryFn || records.length === 0) return [];\n\n    const invoiceNumbers = records.map((r) => r.data['invoiceNumber']).filter(Boolean);\n    if (invoiceNumbers.length === 0) return [];\n\n    const candidates = await ctx.queryFn({\n      entityType: 'invoices',\n      orgId: ctx.orgId,\n      matchFields: ['id', 'invoiceNumber', 'vendorId'],\n      matchValues: { invoiceNumber: invoiceNumbers },\n    });\n\n    if (candidates.length === 0) return [];\n\n    const byInvoiceNum = new Map<string, Array<Record<string, unknown>>>();\n    for (const c of candidates) {\n      const key = String(c['invoiceNumber']);\n      if (!byInvoiceNum.has(key)) byInvoiceNum.set(key, []);\n      byInvoiceNum.get(key)!.push(c);\n    }\n\n    const conflicts: Conflict[] = [];\n    for (const record of records) {\n      const invNum = record.data['invoiceNumber'] ? String(record.data['invoiceNumber']) : null;\n      if (!invNum || !byInvoiceNum.has(invNum)) continue;\n\n      const matchList = byInvoiceNum.get(invNum)!;\n      const matches: MatchCandidate[] = matchList.map((c) => {\n        let score = 50;\n        const explanations: MatchExplanation[] = [{\n          field: 'invoiceNumber',\n          matchType: 'exact',\n          scoreContribution: 50,\n          legacyValue: invNum,\n          candidateValue: String(c['invoiceNumber']),\n        }];\n        if (record.data['vendorId'] && String(record.data['vendorId']) === String(c['vendorId'])) {\n          score += 40;\n          explanations.push({\n            field: 'vendorId',\n            matchType: 'exact',\n            scoreContribution: 40,\n            legacyValue: String(record.data['vendorId']),\n            candidateValue: String(c['vendorId']),\n          });\n        }\n        return { entityId: String(c['id']), entity: c, score, explanations };\n      }).sort((a, b) => b.score - a.score);\n\n      conflicts.push({\n        id: crypto.randomUUID(),\n        legacyRecord: record,\n        matches,\n        confidence: matches[0].score >= 90 ? 'high' : 'medium',\n      });\n    }\n\n    return conflicts;\n  }\n}\n\n// ÔöÇÔöÇ Products (sku) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class ProductsConflictDetector implements ConflictDetector {\n  readonly name = 'products_sku';\n  readonly entityType: EntityType = 'products';\n  readonly matchKeys = ['sku'] as const;\n\n  async detectBulk(\n    records: TransformedRecord[],\n    ctx: DetectorContext\n  ): Promise<Conflict[]> {\n    if (!ctx.queryFn || records.length === 0) return [];\n\n    const skus = records.map((r) => r.data['sku']).filter(Boolean);\n    if (skus.length === 0) return [];\n\n    const candidates = await ctx.queryFn({\n      entityType: 'products',\n      orgId: ctx.orgId,\n      matchFields: ['id', 'sku', 'name'],\n      matchValues: { sku: skus },\n    });\n\n    if (candidates.length === 0) return [];\n\n    const bySku = new Map<string, Record<string, unknown>>();\n    for (const c of candidates) {\n      bySku.set(String(c['sku']), c);\n    }\n\n    const conflicts: Conflict[] = [];\n    for (const record of records) {\n      const sku = record.data['sku'] ? String(record.data['sku']) : null;\n      if (!sku || !bySku.has(sku)) continue;\n\n      const candidate = bySku.get(sku)!;\n      conflicts.push({\n        id: crypto.randomUUID(),\n        legacyRecord: record,\n        matches: [{\n          entityId: String(candidate['id']),\n          entity: candidate,\n          score: 100,\n          explanations: [{\n            field: 'sku',\n            matchType: 'exact',\n            scoreContribution: 100,\n            legacyValue: sku,\n            candidateValue: String(candidate['sku']),\n          }],\n        }],\n        confidence: 'high',\n      });\n    }\n\n    return conflicts;\n  }\n}\n\n// ÔöÇÔöÇ Fallback (no conflict detection, lineage-only) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class NoConflictDetector implements ConflictDetector {\n  readonly name = 'no_conflict';\n  readonly matchKeys: readonly string[] = [];\n\n  constructor(public readonly entityType: EntityType) { }\n\n  async detectBulk(): Promise<Conflict[]> {\n    return [];\n  }\n}\n\n// ÔöÇÔöÇ Total registry (canonized + exhaustive) ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport const CONFLICT_DETECTOR_REGISTRY: Record<string, ConflictDetector> = {\n  contacts: new ContactsConflictDetector(),\n  companies: new NoConflictDetector('companies'),\n  invoices: new InvoicesConflictDetector(),\n  products: new ProductsConflictDetector(),\n  sites: new NoConflictDetector('sites'),\n  currencies: new NoConflictDetector('currencies'),\n  uom: new NoConflictDetector('uom'),\n};\n\nexport function getConflictDetector(entityType: EntityType): ConflictDetector {\n  const detector = CONFLICT_DETECTOR_REGISTRY[entityType];\n  if (!detector) {\n    throw new Error(`No conflict detector for entity type: ${entityType}`);\n  }\n  if (detector.entityType !== entityType) {\n    throw new Error(\n      `Detector entity type mismatch: expected ${entityType}, got ${detector.entityType}`\n    );\n  }\n  return detector;\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\strategies\\fuzzy-name-matcher.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\strategies\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\transforms\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\transforms\\transform-chain.ts","messages":[{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async method 'transform' has no 'await' expression.","line":49,"column":3,"nodeType":"FunctionExpression","messageId":"missingAwait","endLine":49,"endColumn":18,"suggestions":[{"messageId":"removeAsync","fix":{"range":[1059,1194],"text":"transform(\n    value: unknown,\n    fieldName: string,\n    dataType: DataType,\n    context: TransformContext\n  ): unknown"},"desc":"Remove 'async'."}]}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import { parsePhoneNumberFromString, type CountryCode } from 'libphonenumber-js';\n\n/**\n * DET-05: Transform steps execute in monotonic order, coercion last.\n *\n * Steps have an explicit `order` field. TransformChain sorts by order\n * on every addStep() call so insertion order doesn't matter.\n */\n\nexport type DataType =\n  | 'short_text'\n  | 'long_text'\n  | 'email'\n  | 'phone'\n  | 'integer'\n  | 'decimal'\n  | 'boolean'\n  | 'date'\n  | 'datetime'\n  | 'json'\n  | 'uuid';\n\nexport interface TransformContext {\n  entityType: string;\n  orgId: string;\n  locale?: string;\n}\n\nexport interface TransformStep {\n  readonly name: string;\n  readonly order: number;\n  transform(value: unknown, context: TransformContext): unknown;\n  canHandle(fieldName: string, dataType: DataType): boolean;\n}\n\nexport class TransformChain {\n  private steps: TransformStep[] = [];\n\n  addStep(step: TransformStep): this {\n    this.steps.push(step);\n    this.steps.sort((a, b) => a.order - b.order);\n    return this;\n  }\n\n  getSteps(): readonly TransformStep[] {\n    return this.steps;\n  }\n\n  async transform(\n    value: unknown,\n    fieldName: string,\n    dataType: DataType,\n    context: TransformContext\n  ): Promise<unknown> {\n    let result = value;\n    for (const step of this.steps) {\n      if (step.canHandle(fieldName, dataType)) {\n        result = step.transform(result, context);\n      }\n    }\n    return result;\n  }\n}\n\n// ÔöÇÔöÇ Concrete steps ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport class TrimWhitespaceStep implements TransformStep {\n  readonly name = 'trim_whitespace';\n  readonly order = 10;\n\n  canHandle(_fieldName: string, dataType: DataType): boolean {\n    return dataType === 'short_text' || dataType === 'long_text' || dataType === 'email';\n  }\n\n  transform(value: unknown): unknown {\n    return typeof value === 'string' ? value.trim() : value;\n  }\n}\n\nexport class NormalizeWhitespaceStep implements TransformStep {\n  readonly name = 'normalize_whitespace';\n  readonly order = 20;\n\n  canHandle(_fieldName: string, dataType: DataType): boolean {\n    return dataType === 'short_text' || dataType === 'long_text';\n  }\n\n  transform(value: unknown): unknown {\n    return typeof value === 'string' ? value.replace(/\\s+/g, ' ') : value;\n  }\n}\n\nexport class PhoneNormalizeStep implements TransformStep {\n  readonly name = 'phone_normalize';\n  readonly order = 30;\n  private readonly defaultRegion: CountryCode;\n\n  constructor(defaultRegion: CountryCode = 'MY') {\n    this.defaultRegion = defaultRegion;\n  }\n\n  canHandle(_fieldName: string, dataType: DataType): boolean {\n    return dataType === 'phone';\n  }\n\n  transform(value: unknown): unknown {\n    if (typeof value !== 'string' || value.trim() === '') return value;\n    try {\n      const parsed = parsePhoneNumberFromString(value, this.defaultRegion);\n      if (!parsed?.isValid()) return null;\n      return parsed.format('E.164');\n    } catch {\n      return null;\n    }\n  }\n}\n\nconst GMAIL_DOMAINS = new Set(['gmail.com', 'googlemail.com']);\n\nexport class EmailNormalizeStep implements TransformStep {\n  readonly name = 'email_normalize';\n  readonly order = 40;\n\n  canHandle(_fieldName: string, dataType: DataType): boolean {\n    return dataType === 'email';\n  }\n\n  transform(value: unknown): unknown {\n    if (typeof value !== 'string') return value;\n    const trimmed = value.toLowerCase().trim();\n    const atIdx = trimmed.indexOf('@');\n    if (atIdx === -1) return trimmed;\n\n    let local = trimmed.slice(0, atIdx);\n    const domain = trimmed.slice(atIdx + 1);\n\n    if (GMAIL_DOMAINS.has(domain)) {\n      // Strip dots from local part\n      local = local.replace(/\\./g, '');\n      // Strip + alias\n      const plusIdx = local.indexOf('+');\n      if (plusIdx !== -1) {\n        local = local.slice(0, plusIdx);\n      }\n    }\n\n    return `${local}@${domain}`;\n  }\n}\n\nexport class TypeCoercionStep implements TransformStep {\n  readonly name = 'type_coercion';\n  readonly order = 100; // ALWAYS LAST\n\n  canHandle(): boolean {\n    return true;\n  }\n\n  transform(value: unknown, _context: TransformContext): unknown {\n    // Type coercion is a pass-through in the skeleton.\n    // Concrete implementations coerce based on target schema.\n    return value;\n  }\n}\n\n// ÔöÇÔöÇ Factory ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ\n\nexport function buildStandardTransformChain(phoneRegion: CountryCode = 'MY'): TransformChain {\n  return new TransformChain()\n    .addStep(new TrimWhitespaceStep())\n    .addStep(new NormalizeWhitespaceStep())\n    .addStep(new PhoneNormalizeStep(phoneRegion))\n    .addStep(new EmailNormalizeStep())\n    .addStep(new TypeCoercionStep());\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\checkpoint.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\conflict-thresholds.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\cursor.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\legacy-key.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\match-explanation.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\migration-job.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\query.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\record-outcome.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\types\\upsert-plan.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\worker\\index.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\worker\\job-executor.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'EntityType' is defined but never used.","line":23,"column":3,"nodeType":"Identifier","messageId":"unusedVar","endLine":23,"endColumn":13,"suggestions":[{"messageId":"removeUnusedVar","data":{"varName":"EntityType"},"fix":{"range":[686,700],"text":""},"desc":"Remove unused variable \"EntityType\"."}]},{"ruleId":"@typescript-eslint/no-unsafe-assignment","severity":1,"message":"Unsafe assignment of an `any` value.","line":115,"column":15,"nodeType":"VariableDeclarator","messageId":"anyAssignment","endLine":115,"endColumn":82},{"ruleId":"@typescript-eslint/no-unsafe-call","severity":1,"message":"Unsafe call of an `any` typed value.","line":115,"column":35,"nodeType":"MemberExpression","messageId":"unsafeCall","endLine":115,"endColumn":67},{"ruleId":"@typescript-eslint/no-explicit-any","severity":1,"message":"Unexpected any. Specify a different type.","line":115,"column":48,"nodeType":"TSAnyKeyword","messageId":"unexpectedAny","endLine":115,"endColumn":51,"suggestions":[{"messageId":"suggestUnknown","fix":{"range":[3144,3147],"text":"unknown"},"desc":"Use `unknown` instead, this will force you to explicitly, and safely assert the type is correct."},{"messageId":"suggestNever","fix":{"range":[3144,3147],"text":"never"},"desc":"Use `never` instead, this is useful when instantiating generic type parameters that you don't need to know the type of."}]},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .transformBatch on an `any` value.","line":115,"column":53,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":115,"endColumn":67},{"ruleId":"@typescript-eslint/no-unsafe-assignment","severity":1,"message":"Unsafe assignment of an `any` value.","line":118,"column":15,"nodeType":"VariableDeclarator","messageId":"anyAssignment","endLine":118,"endColumn":70},{"ruleId":"@typescript-eslint/no-unsafe-call","severity":1,"message":"Unsafe call of an `any` typed value.","line":118,"column":28,"nodeType":"MemberExpression","messageId":"unsafeCall","endLine":118,"endColumn":57},{"ruleId":"@typescript-eslint/no-explicit-any","severity":1,"message":"Unexpected any. Specify a different type.","line":118,"column":41,"nodeType":"TSAnyKeyword","messageId":"unexpectedAny","endLine":118,"endColumn":44,"suggestions":[{"messageId":"suggestUnknown","fix":{"range":[3269,3272],"text":"unknown"},"desc":"Use `unknown` instead, this will force you to explicitly, and safely assert the type is correct."},{"messageId":"suggestNever","fix":{"range":[3269,3272],"text":"never"},"desc":"Use `never` instead, this is useful when instantiating generic type parameters that you don't need to know the type of."}]},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .planUpserts on an `any` value.","line":118,"column":46,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":118,"endColumn":57},{"ruleId":"@typescript-eslint/no-unsafe-assignment","severity":1,"message":"Unsafe assignment of an `any` value.","line":121,"column":15,"nodeType":"VariableDeclarator","messageId":"anyAssignment","endLine":121,"endColumn":66},{"ruleId":"@typescript-eslint/no-unsafe-call","severity":1,"message":"Unsafe call of an `any` typed value.","line":121,"column":34,"nodeType":"MemberExpression","messageId":"unsafeCall","endLine":121,"endColumn":60},{"ruleId":"@typescript-eslint/no-explicit-any","severity":1,"message":"Unexpected any. Specify a different type.","line":121,"column":47,"nodeType":"TSAnyKeyword","messageId":"unexpectedAny","endLine":121,"endColumn":50,"suggestions":[{"messageId":"suggestUnknown","fix":{"range":[3396,3399],"text":"unknown"},"desc":"Use `unknown` instead, this will force you to explicitly, and safely assert the type is correct."},{"messageId":"suggestNever","fix":{"range":[3396,3399],"text":"never"},"desc":"Use `never` instead, this is useful when instantiating generic type parameters that you don't need to know the type of."}]},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .loadPlan on an `any` value.","line":121,"column":52,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":121,"endColumn":60},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .created on an `any` value.","line":125,"column":45,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":125,"endColumn":52},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .updated on an `any` value.","line":126,"column":45,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":126,"endColumn":52},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .skipped on an `any` value.","line":127,"column":45,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":127,"endColumn":52},{"ruleId":"@typescript-eslint/no-unsafe-member-access","severity":1,"message":"Unsafe member access .failed on an `any` value.","line":128,"column":44,"nodeType":"Identifier","messageId":"unsafeMemberExpression","endLine":128,"endColumn":50},{"ruleId":"@typescript-eslint/no-explicit-any","severity":1,"message":"Unexpected any. Specify a different type.","line":141,"column":41,"nodeType":"TSAnyKeyword","messageId":"unexpectedAny","endLine":141,"endColumn":44,"suggestions":[{"messageId":"suggestUnknown","fix":{"range":[4158,4161],"text":"unknown"},"desc":"Use `unknown` instead, this will force you to explicitly, and safely assert the type is correct."},{"messageId":"suggestNever","fix":{"range":[4158,4161],"text":"never"},"desc":"Use `never` instead, this is useful when instantiating generic type parameters that you don't need to know the type of."}]},{"ruleId":"@typescript-eslint/no-explicit-any","severity":1,"message":"Unexpected any. Specify a different type.","line":225,"column":42,"nodeType":"TSAnyKeyword","messageId":"unexpectedAny","endLine":225,"endColumn":45,"suggestions":[{"messageId":"suggestUnknown","fix":{"range":[6694,6697],"text":"unknown"},"desc":"Use `unknown` instead, this will force you to explicitly, and safely assert the type is correct."},{"messageId":"suggestNever","fix":{"range":[6694,6697],"text":"never"},"desc":"Use `never` instead, this is useful when instantiating generic type parameters that you don't need to know the type of."}]}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":18,"fixableErrorCount":0,"fixableWarningCount":0,"source":"\nimport {\n  migrationJobs,\n  migrationConflicts,\n  migrationConflictResolutions,\n  migrationReports,\n} from 'afena-database';\nimport { and, eq } from 'drizzle-orm';\n\nimport { buildSignedReport } from '../audit/signed-report.js';\nimport { SqlMigrationPipeline } from '../pipeline/sql-migration-pipeline.js';\n\nimport { RateLimiter } from './rate-limiter.js';\n\nimport type { LegacyAdapter } from '../adapters/legacy-adapter.js';\nimport type { ReportInputs } from '../audit/signed-report.js';\nimport type { SqlPipelineConfig } from '../pipeline/sql-migration-pipeline.js';\nimport type { Cursor } from '../types/cursor.js';\nimport type {\n  MigrationJob,\n  MigrationContext,\n  MigrationResult,\n  EntityType,\n} from '../types/migration-job.js';\nimport type { DbInstance } from 'afena-database';\n\n/**\n * Job execution result ÔÇö extends MigrationResult with timing info.\n */\nexport interface JobExecutionResult extends MigrationResult {\n  durationMs: number;\n  batchesProcessed: number;\n  reportHash?: string;\n}\n\n/**\n * Job executor ÔÇö orchestrates a full migration job lifecycle.\n *\n * 1. Mark job as running\n * 2. Run preflight gates\n * 3. Extract ÔåÆ Transform ÔåÆ Plan ÔåÆ Load in batches\n * 4. Run postflight gates\n * 5. Generate signed report\n * 6. Mark job as completed/failed\n */\nexport class JobExecutor {\n  constructor(\n    private readonly db: DbInstance,\n    private readonly legacyAdapter: LegacyAdapter,\n    private readonly pipelineConfig: Omit<SqlPipelineConfig, 'db'>\n  ) { }\n\n  async execute(\n    job: MigrationJob,\n    context: MigrationContext\n  ): Promise<JobExecutionResult> {\n    const startTime = Date.now();\n    const rateLimiter = job.rateLimit\n      ? new RateLimiter(job.rateLimit, job.batchSize)\n      : null;\n\n    const pipeline = new SqlMigrationPipeline(job, context, {\n      ...this.pipelineConfig,\n      db: this.db,\n    });\n\n    const result: JobExecutionResult = {\n      recordsProcessed: 0,\n      recordsCreated: 0,\n      recordsUpdated: 0,\n      recordsMerged: 0,\n      recordsSkipped: 0,\n      recordsFailed: 0,\n      recordsManualReview: 0,\n      durationMs: 0,\n      batchesProcessed: 0,\n    };\n\n    try {\n      // 1. Mark job as running\n      await this.db\n        .update(migrationJobs)\n        .set({ status: 'running', startedAt: new Date() })\n        .where(eq(migrationJobs.id, job.id));\n\n      // 2. Extract + process in batches\n      let cursor: Cursor = job.checkpointCursor ?? null;\n      let batchCount = 0;\n\n      while (true) {\n        // Check max runtime\n        if (job.maxRuntimeMs && Date.now() - startTime > job.maxRuntimeMs) {\n          // Save checkpoint and exit gracefully\n          await this.saveCheckpoint(job.id, cursor);\n          break;\n        }\n\n        // Rate limit\n        if (rateLimiter) {\n          await rateLimiter.acquire(job.batchSize);\n        }\n\n        // Extract batch from legacy source\n        const batch = await this.legacyAdapter.extractBatch(\n          job.entityType,\n          job.batchSize,\n          cursor\n        );\n\n        if (batch.records.length === 0) {\n          break; // Done\n        }\n\n        // Transform\n        const transformed = await (pipeline as any).transformBatch(batch.records);\n\n        // Plan upserts (bulk prefetch, no N+1)\n        const plan = await (pipeline as any).planUpserts(transformed);\n\n        // Load plan (reservation-first creates)\n        const loadResult = await (pipeline as any).loadPlan(plan);\n\n        // Accumulate results\n        result.recordsProcessed += batch.records.length;\n        result.recordsCreated += loadResult.created.length;\n        result.recordsUpdated += loadResult.updated.length;\n        result.recordsSkipped += loadResult.skipped.length;\n        result.recordsFailed += loadResult.failed.length;\n        batchCount++;\n\n        // Update checkpoint cursor\n        cursor = batch.nextCursor;\n        await this.saveCheckpoint(job.id, cursor);\n\n        // Update progress in DB\n        await this.db\n          .update(migrationJobs)\n          .set({\n            recordsSuccess: result.recordsCreated + result.recordsUpdated,\n            recordsFailed: result.recordsFailed,\n            checkpointCursor: cursor as any,\n          })\n          .where(eq(migrationJobs.id, job.id));\n\n        if (!cursor) {\n          break; // No more batches\n        }\n      }\n\n      result.batchesProcessed = batchCount;\n      result.durationMs = Date.now() - startTime;\n\n      // 3. Generate signed report\n      const schema = await this.legacyAdapter.getSchema(job.entityType);\n      const transformSteps = pipeline.getTransformChain().getSteps().map((s) => ({\n        name: s.name,\n        order: s.order,\n      }));\n      const detector = pipeline.getConflictDetector();\n\n      const reportInputs: ReportInputs = {\n        job: {\n          ...job,\n          status: 'completed',\n          checkpointCursor: cursor,\n          recordsSuccess: result.recordsCreated + result.recordsUpdated,\n          recordsFailed: result.recordsFailed,\n        },\n        result,\n        sourceSchemaFingerprint: JSON.stringify(schema.columns.map((c) => `${c.name}:${c.type}`)),\n        transformSteps,\n        conflictDetectorName: detector.entityType,\n        conflictDetectorMatchKeys: detector.matchKeys,\n        mergeEvidenceIds: await this.queryMergeEvidenceIds(job.id, context.orgId),\n        manualReviewIds: await this.queryManualReviewIds(job.id, context.orgId),\n      };\n\n      const report = buildSignedReport(reportInputs);\n      result.reportHash = report.reportHash;\n\n      // Persist report to migration_reports table\n      try {\n        await this.db.insert(migrationReports).values({\n          jobId: job.id,\n          reportData: report,\n          reportHash: report.reportHash,\n        });\n      } catch {\n        // Report persistence failure is non-fatal\n      }\n\n      // 4. Mark job as completed\n      await this.db\n        .update(migrationJobs)\n        .set({\n          status: 'completed',\n          completedAt: new Date(),\n          recordsSuccess: result.recordsCreated + result.recordsUpdated,\n          recordsFailed: result.recordsFailed,\n        })\n        .where(eq(migrationJobs.id, job.id));\n\n    } catch (error) {\n      result.durationMs = Date.now() - startTime;\n\n      // Mark job as failed\n      await this.db\n        .update(migrationJobs)\n        .set({\n          status: 'failed',\n          completedAt: new Date(),\n          recordsFailed: result.recordsFailed,\n        })\n        .where(eq(migrationJobs.id, job.id));\n\n      throw error;\n    }\n\n    return result;\n  }\n\n  private async saveCheckpoint(jobId: string, cursor: Cursor): Promise<void> {\n    await this.db\n      .update(migrationJobs)\n      .set({ checkpointCursor: cursor as any })\n      .where(eq(migrationJobs.id, jobId));\n  }\n\n  // Nit B: Job-scoped evidence queries for signed report\n  private async queryMergeEvidenceIds(jobId: string, orgId: string): Promise<string[]> {\n    try {\n      const rows = await this.db\n        .select({ id: migrationConflictResolutions.id })\n        .from(migrationConflictResolutions)\n        .innerJoin(\n          migrationConflicts,\n          eq(migrationConflictResolutions.conflictId, migrationConflicts.id)\n        )\n        .where(\n          and(\n            eq(migrationConflicts.orgId, orgId),\n            eq(migrationConflicts.migrationJobId, jobId),\n            eq(migrationConflictResolutions.decision, 'merged')\n          )\n        );\n      return rows.map((r) => r.id);\n    } catch {\n      return [];\n    }\n  }\n\n  private async queryManualReviewIds(jobId: string, orgId: string): Promise<string[]> {\n    try {\n      const rows = await this.db\n        .select({ id: migrationConflicts.id })\n        .from(migrationConflicts)\n        .where(\n          and(\n            eq(migrationConflicts.orgId, orgId),\n            eq(migrationConflicts.migrationJobId, jobId),\n            eq(migrationConflicts.resolution, 'manual_review')\n          )\n        );\n      return rows.map((r) => r.id);\n    } catch {\n      return [];\n    }\n  }\n}\n","usedDeprecatedRules":[]},{"filePath":"C:\\AI-BOS\\AFENDA-NEXUS\\packages\\migration\\src\\worker\\rate-limiter.ts","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[]}]
